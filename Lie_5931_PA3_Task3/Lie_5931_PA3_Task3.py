# -*- coding: utf-8 -*-
"""Lie_5931_PA3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GF0FfV3xZWD2OV8KEsbQeLiMWHoFvUY-
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import csv
import itertools
import operator
import numpy as np
import sys
from datetime import datetime
import matplotlib.pyplot as plt

# %cd /content/drive/My\ Drive/5931_PA3
# %ls

!pip3 install nltk

import nltk
import ssl

try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

nltk.download()

import matplotlib.pyplot as plt

def generate_loss_epoch_plt(losses):
  ''' generates loss vs epoch plot based off of list of losses generted from training'''
  x_plt = []  # holds x values
  y_plt = []  # holds y values

  epoch_ct = 0  # need epoch_ct to keep track of which epoch the loss belongs to
  # for each loss add the epoch and appropriate loss from tuple to x and y
  for loss in losses:
    x_plt.append(epoch_ct)
    y_plt.append(loss[1])
    epoch_ct += 1

  plt.plot(x_plt, y_plt)  # plot from x and y lists
  plt.title('Loss vs. Epoch') # title 
  plt.ylabel('Loss') # y axis title
  plt.xlabel('Epoch') # x axis title
  plt.show()

vocabulary_size = 8000
unknown_token = "UNKNOWN"
sentence_start_token = "SENTENCE_START"
sentence_end_token = "SENTENCE_END"
num_ascii_char = 256

"""### From file specified, from each paragraph, tokenize into sentences and add sentence_start_token and sentence_end_token into sentences. Get rid of the newline character and replace with a space and append to the sentence_list. Finally, report how many sentences there are."""

print("Reading .txt file...")

sentences = []
sentence_list = []
filename = "SciFi_Three.txt"
with open(filename, "r") as f:
    
    # Split all paragraphs by two newline characters
    all_content = f.read()
    paragraphs = all_content.split("\n\n")
    
    # For each paragraph, tokenize sentences and then place in sentence_list
    for paragraph in paragraphs:

        sentences = nltk.sent_tokenize(paragraph.lower())
        
        sentences = ["%s %s %s" % (sentence_start_token, x, sentence_end_token) for x in sentences]

        for sentence in sentences:
            sentence = sentence.replace("\n", " ")
            sentence_list.append(sentence)
            
print("Parsed %d sentences." % (len(sentence_list)))  # report how many sentences

# Display how the sentences 0-99 in sentence_list
sentence_list[0:100]

"""### The below code is used for part a). It concatenates every two consecutive sentences together to approximately double the sentence length that is being fed into our network for training."""

count = 0
sent = ""
token_sent_double = []
for sentence in sentence_list:
  sent += sentence # set sent to two consecutive sentences
  sent = sent.replace(' SENTENCE_ENDSENTENCE_START ', ' ') # need to get rid of start and end in middle after concatenating

  count += 1
  if count % 2 == 0:
    token_sent_double.append(sent) # append to list
    sent = ""
    count = 0

"""### The below code is used for part b). It halves every sentence to halve the sentence length that is being fed into our network for training."""

token_sent_half = []
for sentence in sentence_list:
  sentence = nltk.word_tokenize(sentence) # tokenize into list of separate words
  first_half, second_half = (sentence[:round(len(sentence)/2)]), (sentence[round(len(sentence)/2):]) # split string into first and second halves
  first_half.append('SENTENCE_END') # add sentence end to first half at end
  second_half.insert(0, 'SENTENCE_START') # add sentence start to second half at beginning
  token_sent_half.append(" ".join(first_half)) # must join before appending to rejoin into sentence string
  token_sent_half.append(" ".join(second_half))

"""### Get tokenized sentence lists for each separate part"""

# for first part
tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentence_list]

# for second part with double length sentences
tokenized_sentences_double = [nltk.word_tokenize(sent) for sent in token_sent_double]

# for third part with half length sentences
tokenized_sentences_half = [nltk.word_tokenize(sent) for sent in token_sent_half]

print((str(len(tokenized_sentences))) + " tokenized sentences for first part.")

# Note how number of sentences in double the length is half the number compared to original
print((str(len(tokenized_sentences_double))) + " tokenized sentences for second part.")

# Note how number of sentences in half the length is double the number compared to original
print((str(len(tokenized_sentences_half))) + " tokenized sentences for third part.")

print(tokenized_sentences[6])

print(tokenized_sentences_double[6])

print(tokenized_sentences_half[6])

word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))

word_freq_double = nltk.FreqDist(itertools.chain(*tokenized_sentences_double))

word_freq_half = nltk.FreqDist(itertools.chain(*tokenized_sentences_half))

print("Found %d unique words tokens in tokenized_sentences." % len(word_freq.items()))

print("Found %d unique words tokens in tokenized_sentences_double." % len(word_freq_double.items()))

print("Found %d unique words tokens in tokenized_sentences_half." % len(word_freq_half.items()))

word_freq

word_freq_double

word_freq_half

vocab_1 = word_freq.most_common(vocabulary_size-1) # for original 
vocab_2 = word_freq_double.most_common(vocabulary_size-1) # for double
vocab_3 = word_freq_half.most_common(vocabulary_size-1) # for half

# index to word and word to index for original
index_to_word_1 = [x[0] for x in vocab_1]
index_to_word_1.append(unknown_token)
word_to_index_1 = dict([(w,i) for i,w in enumerate(index_to_word_1)])

# index to word and word to index for double
index_to_word_2 = [x[0] for x in vocab_2]
index_to_word_2.append(unknown_token)
word_to_index_2 = dict([(w,i) for i,w in enumerate(index_to_word_2)])

# index to word and word to index for half
index_to_word_3 = [x[0] for x in vocab_3]
index_to_word_3.append(unknown_token)
word_to_index_3 = dict([(w,i) for i,w in enumerate(index_to_word_3)])

word_to_index_1

word_to_index_2

word_to_index_3

print("Using vocabulary size %d." % vocabulary_size)
print("The least frequent word in our original vocabulary is '%s' and appeared %d times." % (vocab_1[-1][0], vocab_1[-1][1]))
print("The least frequent word in our double vocabulary is '%s' and appeared %d times." % (vocab_2[-1][0], vocab_2[-1][1]))
print("The least frequent word in our half vocabulary is '%s' and appeared %d times." % (vocab_3[-1][0], vocab_3[-1][1]))
#word_freq.most_common(10)

print(index_to_word_1[7999])
print(index_to_word_2[7999])
print(index_to_word_3[7999])

# Replace all words not in vocabulary with the unknown token ORIGINAL
for i, sentence in enumerate(tokenized_sentences):
    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sentence]

# Replace all words not in vocabulary with the unknown token DOUBLE
for i, sentence in enumerate(tokenized_sentences_double):
    tokenized_sentences_double[i] = [w if w in word_to_index else unknown_token for w in sentence]

# Replace all words not in vocabulary with the unknown token HALF
for i, sentence in enumerate(tokenized_sentences_half):
    tokenized_sentences_half[i] = [w if w in word_to_index else unknown_token for w in sentence]

print("\nExample sentence: '%s'" % sentence_list[3])
print("\nExample sentence after Pre-processing: '%s'" % tokenized_sentences[3])

"""### Create training data for each part of Task 1"""

# Create the training data for original
XTrain = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])
YTrain = np.asarray([[word_to_index[w] for w in sent [1:]] for sent in tokenized_sentences])

# Create the training data for double
XTrain_double = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences_double])
YTrain_double = np.asarray([[word_to_index[w] for w in sent [1:]] for sent in tokenized_sentences_double])

# Create the training data for half
XTrain_half = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences_half])
YTrain_half = np.asarray([[word_to_index[w] for w in sent [1:]] for sent in tokenized_sentences_half])

XTrain

XTrain_double

XTrain_half

YTrain

YTrain_double

YTrain_half

"""### Show 5 random sentences from XTrain and YTrain, to show how YTrain is just XTrain shifted once to the left"""

import random

for i in range(5):
    idx = random.randint(0, len(XTrain))
    x_example, y_example = XTrain[idx], YTrain[idx]
    print("Sample id %d\n========================\n" % idx)
    print("x:\n%s\n%s" % (" ".join([index_to_word[x] for x in x_example]), x_example))
    print("\ny:\n%s\n%s" % (" ".join([index_to_word[x] for x in y_example]), y_example))

"""### Create the Vanilla RNN Class"""

class RNNVanilla:

    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):

        # Assign instance variables
        self.word_dim = word_dim # size of the vocabulary
        self.hidden_dim = hidden_dim # size of hidden layer
        self.bptt_truncate = bptt_truncate

        # Randomly initialize the network parameters
        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))
        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))
        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))

def softmax(x):
  ''' softmax function '''
  xt = np.exp(x - np.max(x))
  return xt / np.sum(xt)

def forward_propagation(self, x):
    # Total number of time steps
    T = len(x)

    # During forward propagation we save all hidden states in s because we need them later.

    # We add one additional element for the initial hidden, which we set to 0
    s = np.zeros((T + 1, self.hidden_dim))
    s[-1] = np.zeros(self.hidden_dim)

    # The outputs at each time step, Again, we save them for later
    o = np.zeros((T, self.word_dim))

    # For each time step...
    for t in np.arange(T):
        # Note that we are indexing U by x[t]. This is the same as multiplying U with a one-hot vector.
        s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))
        o[t] = softmax(self.V.dot(s[t]))
    return [o, s] # We not only return the calculated outputs, but also the hidden states.
                  # We will use them later to calculate the gradients

# Now make it a member of the class RNNVanilla
RNNVanilla.forward_propagation = forward_propagation

def predict(self, x):
    # Perform forward propagation and return index of the highest score
    o, s = self.forward_propagation(x)
    return np.argmax(o, axis=1)

# Now make it a member of the class RNNVanilla
RNNVanilla.predict = predict

print("Length of XTrain[10] is %d" % (len(XTrain[10])))
np.random.seed(100)

model = RNNVanilla(vocabulary_size)
o, s = model.forward_propagation(XTrain[10])
print(o.shape)
print(o)
# For each word in the sentence, the model made 8000 predictions representing probabilities of the next word
# Note that because we initialized U, V, W to random values these predictions are completely random right now

# The following gives the indices of the highest probability predictions for each word:
predictions = model.predict(XTrain[10])
print(predictions.shape)
print(predictions)
print("index_to_word>")
print('%s'%" ".join([index_to_word[x] for x in predictions]))

def calculate_total_loss(self, x, y):
    L = 0

    # For each sentence
    for i in np.arange(len(y)):
        o, s = self.forward_propagation(x[i])

        # We only care about our prediction of the "correct" words
        correct_word_predictions = o[np.arange(len(y[i])), y[i]]

        # Add to the loss based on how off we were
        L += -1 * sum(np.log(correct_word_predictions))
    return L

def calculate_loss(self, x, y):
    # Divide the total loss by the number of training examples
    N = sum((len(y_i) for y_i in y))
    return self.calculate_total_loss(x,y)/N
    
RNNVanilla.calculate_total_loss = calculate_total_loss
RNNVanilla.calculate_loss = calculate_loss

np.log(vocabulary_size)

XTrain[:100].shape

XTrain.shape

# Limit to 1000 examples to save time
print("Expected Loss for random predictions: %f" % np.log(vocabulary_size))
print("Actual loss: %f" % model.calculate_loss(XTrain[:1000], YTrain[:1000]))

def bptt(self, x, y):
    T = len(y)
    # Perform forward propagation
    o, s = self.forward_propagation(x)
    # We accumulate the gradients in these variables
    dLdU = np.zeros(self.U.shape)
    dLdV = np.zeros(self.V.shape)
    dLdW = np.zeros(self.W.shape)
    delta_o = o
    delta_o[np.arange(len(y)), y] -= 1.
    # For each output backwards...
    for t in np.arange(T)[::-1]:
        dLdV += np.outer(delta_o[t], s[t].T)

        # Initial delta calculation
        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))

        # Backpropagation through time (for at most self.bptt_truncate steps)
        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:

            # print "Backpropagation step t=%d bptt step=%d " % (t, bptt_step)
            dLdW += np.outer(delta_t, s[bptt_step-1])              
            dLdU[:,x[bptt_step]] += delta_t

            # Update delta for next step
            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)
    return [dLdU, dLdV, dLdW]
    

RNNVanilla.bptt = bptt

def gradient_check(self, x, y, h=0.001, error_threshold=0.01):
    # Calculate the gradients using backpropagation. We want to check if these are correct.
    bptt_gradients = self.bptt(x, y)

    # List of all parameters we want to check.
    model_parameters = ['U', 'V', 'W']

    # Gradient check for each parameter
    for pidx, pname in enumerate(model_parameters):
        # Get the actual parameter value from the mode, e.g. model.W
        parameter = operator.attrgetter(pname)(self)
        print("Performing gradient check for parameter %s with size %d." % (pname, np.prod(parameter.shape)))
        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...
        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])
        while not it.finished:
            ix = it.multi_index
            # Save the original value so we can reset it later
            original_value = parameter[ix]
            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)
            parameter[ix] = original_value + h
            gradplus = self.calculate_total_loss([x], [y])
            parameter[ix] = original_value - h
            gradminus = self.calculate_total_loss([x], [y])
            estimated_gradient = (gradplus - gradminus)/(2*h)
            # Reset parameter to original value
            parameter[ix] = original_value
            # The gradient for this parameter calculated using backpropagation
            backprop_gradient = bptt_gradients[pidx][ix]
            # calculate The relative error: (|x - y|/(|x| + |y|))
            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))
            # If the error is to large fail the gradient check
            if relative_error > error_threshold:
                print ("Gradient Check ERROR: parameter=%s ix=%s" % (pname, ix))
                print ("+h Loss: %f" % gradplus)
                print ("-h Loss: %f" % gradminus)
                print ("Estimated_gradient: %f" % estimated_gradient)
                print ("Backpropagation gradient: %f" % backprop_gradient)
                print ("Relative Error: %f" % relative_error)
                return
            it.iternext()
        print ("Gradient check for parameter %s passed." % (pname))

RNNVanilla.gradient_check = gradient_check

# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.
grad_check_vocab_size = 100
np.random.seed(10)
model = RNNVanilla(grad_check_vocab_size, 10, bptt_truncate=1000)
model.gradient_check([0,1,2,3], [1,2,3,4])

# Performs one step of SGD.
def numpy_sdg_step(self, x, y, learning_rate):
    # Calculate the gradients
    dLdU, dLdV, dLdW = self.bptt(x, y)
    # Change parameters according to gradients and learning rate
    self.U -= learning_rate * dLdU
    self.V -= learning_rate * dLdV
    self.W -= learning_rate * dLdW
    
RNNVanilla.sgd_step = numpy_sdg_step

# Outer SGD Loop
# - model: The RNN model instance
# - X_train: The training data set
# - y_train: The training data labels
# - learning_rate: Initial learning rate for SGD
# - nepoch: Number of times to iterate through the complete dataset
# - evaluate_loss_after: Evaluate the loss after this many epochs
def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):
    # We keep track of the losses so we can plot them later
    losses = []
    num_examples_seen = 0
    for epoch in range(nepoch):
        # Optionally evaluate the loss
        if (epoch % evaluate_loss_after == 0):
            loss = model.calculate_loss(X_train, y_train)
            losses.append((num_examples_seen, loss))
            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            print ("%s: Loss after num_examples_seen=%d epoch=%d: %f" % (time, num_examples_seen, epoch, loss))
            # Adjust the learning rate if loss increases
            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):
                learning_rate = learning_rate * 0.5 
                print ("Setting learning rate to %f" % learning_rate)
            sys.stdout.flush()
        # For each training example...
        for i in range(len(y_train)):
            # One SGD step
            model.sgd_step(X_train[i], y_train[i], learning_rate)
            num_examples_seen += 1
    return losses

# Commented out IPython magic to ensure Python compatibility.
np.random.seed(10)
model = RNNVanilla(vocabulary_size)
# %timeit model.sgd_step(XTrain[10], YTrain[10], learning_rate=0.005)

"""# 5 Breakpoints

## We train below at 5 different breakpoints: 6, 12, 18, 24 and 30 epochs to show how well our network learns over time.

## We feed our network just a subset of our data (0-99) of XTrain and YTrain.

## After each breakpoint trained, we generate 10 sentences.
"""

def generate_sentence(model):
    # We start the sentence with the start token
    new_sentence = [word_to_index[sentence_start_token]]
    
    # Repeat until we get an end token
    while not new_sentence[-1] == word_to_index[sentence_end_token]:
        next_word_probs = model.forward_propagation(new_sentence)
        sampled_word = word_to_index[unknown_token]
        # We don't want to sample unknown words
        while sampled_word == word_to_index[unknown_token]:
            samples = np.random.multinomial(1, next_word_probs[0][-1])
            sampled_word = np.argmax(samples)
        new_sentence.append(sampled_word)
    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]
    return sentence_str

def generate_sentences_with_model(model, num_sentences, senten_min_length, model_num):
  if model_num != 0:
    # Generate 10 sentences for each trained model to compare 
    print ("Generating sentences for model " + str(model_num) + " ...")
  for i in range(num_sentences):
    sent = []
      # We want long sentences, not sentences with one or two words
    while len(sent) < senten_min_length:
      sent = generate_sentence(model)
    print ("Sentence # %d of length %d" % ((i+1),  len(sent)))
    print (" ".join(sent))

num_sentences = 10
senten_min_length = 7
breakpoint_size = 6

np.random.seed(10)
# Train on a small subset of the data to see what happens for first 6 epochs
model = RNNVanilla(vocabulary_size)
losses = train_with_sgd(model, XTrain[:100], YTrain[:100], nepoch=breakpoint_size, evaluate_loss_after=1)

model_num = 1
generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)

np.random.seed(10)
# Next 6 epochs
losses_temp = train_with_sgd(model, XTrain[:100], YTrain[:100], nepoch=breakpoint_size, evaluate_loss_after=1)
for loss in losses_temp:
  losses.append(loss)

model_num = 2
generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)

np.random.seed(10)
# Next 6 epochs
losses_temp = train_with_sgd(model, XTrain[:100], YTrain[:100], nepoch=breakpoint_size, evaluate_loss_after=1)
for loss in losses_temp:
  losses.append(loss)

model_num = 3
generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)

np.random.seed(10)
# Next 6 epochs
losses_temp = train_with_sgd(model, XTrain[:100], YTrain[:100], nepoch=breakpoint_size, evaluate_loss_after=1)
for loss in losses_temp:
  losses.append(loss)

model_num = 4
generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)

np.random.seed(10)
# Next 6 epochs
losses_temp = train_with_sgd(model, XTrain[:100], YTrain[:100], nepoch=breakpoint_size, evaluate_loss_after=1)
for loss in losses_temp:
  losses.append(loss)

model_num = 5
generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)

# Print losses for final trained model
for loss in losses:
    print("Losses for final trained model: " + str(loss))

# Generate loss vs epoch plot from final trained model
generate_loss_epoch_plt(losses)

"""### Part a: We double then halve the original number of hidden units.

### We then display a loss vs epoch plot and the text sampling results.
"""

np.random.seed(10)
# Train on a small subset of the data to see what happens for hidden dimension of 100 and 
# 30 epochs
model = RNNVanilla(vocabulary_size, hidden_dim=200)
losses = train_with_sgd(model, XTrain[:100], YTrain[:100], nepoch=30, evaluate_loss_after=1)



model_num = 0
generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)

# Print losses for final trained model
for loss in losses:
    print("Losses for final trained model: " + str(loss))

# Generate loss vs epoch plot from final trained model
generate_loss_epoch_plt(losses)

np.random.seed(10)
# Train on a small subset of the data to see what happens for hidden dimension of 100 and 
# 30 epochs
model = RNNVanilla(vocabulary_size, hidden_dim=50)
losses = train_with_sgd(model, XTrain[:100], YTrain[:100], nepoch=30, evaluate_loss_after=1)



model_num = 0
generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)

# Print losses for final trained model
for loss in losses:
    print("Losses for final trained model: " + str(loss))

# Generate loss vs epoch plot from final trained model
generate_loss_epoch_plt(losses)

"""### Part b: We double then halve the original length of sentences.

### We then display a loss vs epoch plot and the text sampling results.
"""

np.random.seed(10)
# Train on a small subset of the data to see what happens for hidden dimension of 100 and 
# 30 epochs
model = RNNVanilla(vocabulary_size, hidden_dim=100)
losses = train_with_sgd(model, XTrain_double[:100], YTrain_double[:100], nepoch=30, evaluate_loss_after=1)



model_num = 0
generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)

# Print losses for final trained model
for loss in losses:
    print("Losses for final trained model: " + str(loss))

# Generate loss vs epoch plot from final trained model
generate_loss_epoch_plt(losses)

np.random.seed(10)
# Train on a small subset of the data to see what happens for hidden dimension of 100 and 
# 30 epochs
model = RNNVanilla(vocabulary_size, hidden_dim=100)
losses = train_with_sgd(model, XTrain_half[:100], YTrain_half[:100], nepoch=30, evaluate_loss_after=1)



model_num = 0
generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)

# Print losses for final trained model
for loss in losses:
    print("Losses for final trained model: " + str(loss))

# Generate loss vs epoch plot from final trained model
generate_loss_epoch_plt(losses)
