{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Lie_5931_PA3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tobby-lie/Recurrent-Neural-Network-from-Scratch/blob/master/Lie_5931_PA3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2Ny7UiB1dZy",
        "colab_type": "code",
        "outputId": "984158c2-9d99-4f1a-d70f-44f2bc0aaa1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16drKxE11Zae",
        "colab_type": "code",
        "outputId": "4dbe55ed-0dd2-4f5d-f1cc-a833d36228c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import csv\n",
        "import itertools\n",
        "import operator\n",
        "import numpy as np\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%cd /content/drive/My\\ Drive/5931_PA3\n",
        "%ls\n",
        "\n",
        "!pip3 install nltk"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/5931_PA3\n",
            "Lie_5931_PA3.ipynb  SciFi_Three.txt\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kks5iR-u1Zag",
        "colab_type": "code",
        "outputId": "69c470e1-b945-4d76-ca06-a253ba5b7acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "nltk.download()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n",
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt.zip.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvIB0djFIGGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_loss_epoch_plt(losses):\n",
        "  ''' generates loss vs epoch plot based off of list of losses generted from training'''\n",
        "  x_plt = []  # holds x values\n",
        "  y_plt = []  # holds y values\n",
        "\n",
        "  epoch_ct = 0  # need epoch_ct to keep track of which epoch the loss belongs to\n",
        "  # for each loss add the epoch and appropriate loss from tuple to x and y\n",
        "  for loss in losses:\n",
        "    x_plt.append(epoch_ct)\n",
        "    y_plt.append(loss[1])\n",
        "    epoch_ct += 1\n",
        "\n",
        "  plt.plot(x_plt, y_plt)  # plot from x and y lists\n",
        "  plt.title('Loss vs. Epoch') # title \n",
        "  plt.ylabel('Loss') # y axis title\n",
        "  plt.xlabel('Epoch') # x axis title\n",
        "  plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fetT1SAv1Zai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary_size = 8000\n",
        "unknown_token = \"UNKNOWN\"\n",
        "sentence_start_token = \"SENTENCE_START\"\n",
        "sentence_end_token = \"SENTENCE_END\"\n",
        "num_ascii_char = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx8-yOxBNzPo",
        "colab_type": "text"
      },
      "source": [
        "### From file specified, from each paragraph, tokenize into sentences and add sentence_start_token and sentence_end_token into sentences. Get rid of the newline character and replace with a space and append to the sentence_list. Finally, report how many sentences there are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHcg71dF1Zam",
        "colab_type": "code",
        "outputId": "8795d2a7-b35a-4d34-a33c-390c8ed3aa0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Reading .txt file...\")\n",
        "\n",
        "sentences = []\n",
        "sentence_list = []\n",
        "filename = \"SciFi_Three.txt\"\n",
        "with open(filename, \"r\") as f:\n",
        "    \n",
        "    # Split all paragraphs by two newline characters\n",
        "    all_content = f.read()\n",
        "    paragraphs = all_content.split(\"\\n\\n\")\n",
        "    \n",
        "    # For each paragraph, tokenize sentences and then place in sentence_list\n",
        "    for paragraph in paragraphs:\n",
        "\n",
        "        sentences = nltk.sent_tokenize(paragraph.lower())\n",
        "        \n",
        "        sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.replace(\"\\n\", \" \")\n",
        "            sentence_list.append(sentence)\n",
        "            \n",
        "print(\"Parsed %d sentences.\" % (len(sentence_list)))  # report how many sentences\n",
        "            "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading .txt file...\n",
            "Parsed 25324 sentences.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdlm65Lu1Zao",
        "colab_type": "code",
        "outputId": "d8dc095b-0927-4a7d-e323-ea8b4e27befb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Display how the sentences 0-99 in sentence_list\n",
        "sentence_list[0:100]"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SENTENCE_START the mysterious island SENTENCE_END',\n",
              " 'SENTENCE_START by jules verne SENTENCE_END',\n",
              " 'SENTENCE_START 1874 SENTENCE_END',\n",
              " 'SENTENCE_START  part 1--dropped from the clouds SENTENCE_END',\n",
              " 'SENTENCE_START chapter 1 SENTENCE_END',\n",
              " 'SENTENCE_START “are we rising again?” “no. SENTENCE_END',\n",
              " 'SENTENCE_START on the contrary.” “are we descending?”  “worse than that, captain! SENTENCE_END',\n",
              " 'SENTENCE_START we are falling!” “for heaven’s sake heave out the ballast!” “there! SENTENCE_END',\n",
              " 'SENTENCE_START the last sack is empty!” “does the balloon rise?”  “no!” “i hear a noise like the dashing of waves. SENTENCE_END',\n",
              " 'SENTENCE_START the sea is below the car! SENTENCE_END',\n",
              " 'SENTENCE_START it cannot be more than 500 feet from us!” “overboard with every weight! SENTENCE_END',\n",
              " 'SENTENCE_START ... everything!” SENTENCE_END',\n",
              " 'SENTENCE_START such were the loud and startling words which resounded through the air, above the vast watery desert of the pacific, about four o’clock in the evening of the 23rd of march, 1865. SENTENCE_END',\n",
              " 'SENTENCE_START few can possibly have forgotten the terrible storm from the northeast, in the middle of the equinox of that year. SENTENCE_END',\n",
              " 'SENTENCE_START the tempest raged without intermission from the 18th to the 26th of march. SENTENCE_END',\n",
              " 'SENTENCE_START its ravages were terrible in america, europe, and asia, covering a distance of eighteen hundred miles, and extending obliquely to the equator from the thirty-fifth north parallel to the fortieth south parallel. SENTENCE_END',\n",
              " 'SENTENCE_START towns were overthrown, forests uprooted, coasts devastated by the mountains of water which were precipitated on them, vessels cast on the shore, which the published accounts numbered by hundreds, whole districts leveled by waterspouts which destroyed everything they passed over, several thousand people crushed on land or drowned at sea; such were the traces of its fury, left by this devastating tempest. SENTENCE_END',\n",
              " 'SENTENCE_START it surpassed in disasters those which so frightfully ravaged havana and guadalupe, one on the 25th of october, 1810, the other on the 26th of july, 1825. SENTENCE_END',\n",
              " 'SENTENCE_START but while so many catastrophes were taking place on land and at sea, a drama not less exciting was being enacted in the agitated air. SENTENCE_END',\n",
              " 'SENTENCE_START in fact, a balloon, as a ball might be carried on the summit of a waterspout, had been taken into the circling movement of a column of air and had traversed space at the rate of ninety miles an hour, turning round and round as if seized by some aerial maelstrom. SENTENCE_END',\n",
              " 'SENTENCE_START beneath the lower point of the balloon swung a car, containing five passengers, scarcely visible in the midst of the thick vapor mingled with spray which hung over the surface of the ocean. SENTENCE_END',\n",
              " 'SENTENCE_START whence, it may be asked, had come that plaything of the tempest? SENTENCE_END',\n",
              " 'SENTENCE_START from what part of the world did it rise? SENTENCE_END',\n",
              " 'SENTENCE_START it surely could not have started during the storm. SENTENCE_END',\n",
              " 'SENTENCE_START but the storm had raged five days already, and the first symptoms were manifested on the 18th. SENTENCE_END',\n",
              " 'SENTENCE_START it cannot be doubted that the balloon came from a great distance, for it could not have traveled less than two thousand miles in twenty-four hours. SENTENCE_END',\n",
              " 'SENTENCE_START at any rate the passengers, destitute of all marks for their guidance, could not have possessed the means of reckoning the route traversed since their departure. SENTENCE_END',\n",
              " 'SENTENCE_START it was a remarkable fact that, although in the very midst of the furious tempest, they did not suffer from it. SENTENCE_END',\n",
              " 'SENTENCE_START they were thrown about and whirled round and round without feeling the rotation in the slightest degree, or being sensible that they were removed from a horizontal position. SENTENCE_END',\n",
              " 'SENTENCE_START their eyes could not pierce through the thick mist which had gathered beneath the car. SENTENCE_END',\n",
              " 'SENTENCE_START dark vapor was all around them. SENTENCE_END',\n",
              " 'SENTENCE_START such was the density of the atmosphere that they could not be certain whether it was day or night. SENTENCE_END',\n",
              " 'SENTENCE_START no reflection of light, no sound from inhabited land, no roaring of the ocean could have reached them, through the obscurity, while suspended in those elevated zones. SENTENCE_END',\n",
              " 'SENTENCE_START their rapid descent alone had informed them of the dangers which they ran from the waves. SENTENCE_END',\n",
              " 'SENTENCE_START however, the balloon, lightened of heavy articles, such as ammunition, arms, and provisions, had risen into the higher layers of the atmosphere, to a height of 4,500 feet. SENTENCE_END',\n",
              " 'SENTENCE_START the voyagers, after having discovered that the sea extended beneath them, and thinking the dangers above less dreadful than those below, did not hesitate to throw overboard even their most useful articles, while they endeavored to lose no more of that fluid, the life of their enterprise, which sustained them above the abyss. SENTENCE_END',\n",
              " 'SENTENCE_START the night passed in the midst of alarms which would have been death to less energetic souls. SENTENCE_END',\n",
              " 'SENTENCE_START again the day appeared and with it the tempest began to moderate. SENTENCE_END',\n",
              " 'SENTENCE_START from the beginning of that day, the 24th of march, it showed symptoms of abating. SENTENCE_END',\n",
              " 'SENTENCE_START at dawn, some of the lighter clouds had risen into the more lofty regions of the air. SENTENCE_END',\n",
              " 'SENTENCE_START in a few hours the wind had changed from a hurricane to a fresh breeze, that is to say, the rate of the transit of the atmospheric layers was diminished by half. SENTENCE_END',\n",
              " 'SENTENCE_START it was still what sailors call “a close-reefed topsail breeze,” but the commotion in the elements had none the less considerably diminished. SENTENCE_END',\n",
              " 'SENTENCE_START towards eleven o’clock, the lower region of the air was sensibly clearer. SENTENCE_END',\n",
              " 'SENTENCE_START the atmosphere threw off that chilly dampness which is felt after the passage of a great meteor. SENTENCE_END',\n",
              " 'SENTENCE_START the storm did not seem to have gone farther to the west. SENTENCE_END',\n",
              " 'SENTENCE_START it appeared to have exhausted itself. SENTENCE_END',\n",
              " 'SENTENCE_START could it have passed away in electric sheets, as is sometimes the case with regard to the typhoons of the indian ocean? SENTENCE_END',\n",
              " 'SENTENCE_START but at the same time, it was also evident that the balloon was again slowly descending with a regular movement. SENTENCE_END',\n",
              " 'SENTENCE_START it appeared as if it were, little by little, collapsing, and that its case was lengthening and extending, passing from a spherical to an oval form. SENTENCE_END',\n",
              " 'SENTENCE_START towards midday the balloon was hovering above the sea at a height of only 2,000 feet. SENTENCE_END',\n",
              " 'SENTENCE_START it contained 50,000 cubic feet of gas, and, thanks to its capacity, it could maintain itself a long time in the air, although it should reach a great altitude or might be thrown into a horizontal position. SENTENCE_END',\n",
              " 'SENTENCE_START perceiving their danger, the passengers cast away the last articles which still weighed down the car, the few provisions they had kept, everything, even to their pocket-knives, and one of them, having hoisted himself on to the circles which united the cords of the net, tried to secure more firmly the lower point of the balloon. SENTENCE_END',\n",
              " 'SENTENCE_START it was, however, evident to the voyagers that the gas was failing, and that the balloon could no longer be sustained in the higher regions. SENTENCE_END',\n",
              " 'SENTENCE_START they must infallibly perish! SENTENCE_END',\n",
              " 'SENTENCE_START there was not a continent, nor even an island, visible beneath them. SENTENCE_END',\n",
              " 'SENTENCE_START the watery expanse did not present a single speck of land, not a solid surface upon which their anchor could hold. SENTENCE_END',\n",
              " 'SENTENCE_START it was the open sea, whose waves were still dashing with tremendous violence! SENTENCE_END',\n",
              " 'SENTENCE_START it was the ocean, without any visible limits, even for those whose gaze, from their commanding position, extended over a radius of forty miles. SENTENCE_END',\n",
              " 'SENTENCE_START the vast liquid plain, lashed without mercy by the storm, appeared as if covered with herds of furious chargers, whose white and disheveled crests were streaming in the wind. SENTENCE_END',\n",
              " 'SENTENCE_START no land was in sight, not a solitary ship could be seen. SENTENCE_END',\n",
              " 'SENTENCE_START it was necessary at any cost to arrest their downward course, and to prevent the balloon from being engulfed in the waves. SENTENCE_END',\n",
              " 'SENTENCE_START the voyagers directed all their energies to this urgent work. SENTENCE_END',\n",
              " 'SENTENCE_START but, notwithstanding their efforts, the balloon still fell, and at the same time shifted with the greatest rapidity, following the direction of the wind, that is to say, from the northeast to the southwest. SENTENCE_END',\n",
              " 'SENTENCE_START frightful indeed was the situation of these unfortunate men. SENTENCE_END',\n",
              " 'SENTENCE_START they were evidently no longer masters of the machine. SENTENCE_END',\n",
              " 'SENTENCE_START all their attempts were useless. SENTENCE_END',\n",
              " 'SENTENCE_START the case of the balloon collapsed more and more. SENTENCE_END',\n",
              " 'SENTENCE_START the gas escaped without any possibility of retaining it. SENTENCE_END',\n",
              " 'SENTENCE_START their descent was visibly accelerated, and soon after midday the car hung within 600 feet of the ocean. SENTENCE_END',\n",
              " 'SENTENCE_START it was impossible to prevent the escape of gas, which rushed through a large rent in the silk. SENTENCE_END',\n",
              " 'SENTENCE_START by lightening the car of all the articles which it contained, the passengers had been able to prolong their suspension in the air for a few hours. SENTENCE_END',\n",
              " 'SENTENCE_START but the inevitable catastrophe could only be retarded, and if land did not appear before night, voyagers, car, and balloon must to a certainty vanish beneath the waves. SENTENCE_END',\n",
              " 'SENTENCE_START they now resorted to the only remaining expedient. SENTENCE_END',\n",
              " 'SENTENCE_START they were truly dauntless men, who knew how to look death in the face. SENTENCE_END',\n",
              " 'SENTENCE_START not a single murmur escaped from their lips. SENTENCE_END',\n",
              " 'SENTENCE_START they were determined to struggle to the last minute, to do anything to retard their fall. SENTENCE_END',\n",
              " 'SENTENCE_START the car was only a sort of willow basket, unable to float, and there was not the slightest possibility of maintaining it on the surface of the sea. SENTENCE_END',\n",
              " 'SENTENCE_START two more hours passed and the balloon was scarcely 400 feet above the water. SENTENCE_END',\n",
              " 'SENTENCE_START at that moment a loud voice, the voice of a man whose heart was inaccessible to fear, was heard. SENTENCE_END',\n",
              " 'SENTENCE_START to this voice responded others not less determined. SENTENCE_END',\n",
              " 'SENTENCE_START “is everything thrown out?” “no, here are still 2,000 dollars in gold.” a heavy bag immediately plunged into the sea. SENTENCE_END',\n",
              " 'SENTENCE_START “does the balloon rise?” “a little, but it will not be long before it falls again.” “what still remains to be thrown out?” “nothing.” “yes! SENTENCE_END',\n",
              " 'SENTENCE_START the car!” “let us catch hold of the net, and into the sea with the car.” SENTENCE_END',\n",
              " 'SENTENCE_START this was, in fact, the last and only mode of lightening the balloon. SENTENCE_END',\n",
              " 'SENTENCE_START the ropes which held the car were cut, and the balloon, after its fall, mounted 2,000 feet. SENTENCE_END',\n",
              " 'SENTENCE_START the five voyagers had hoisted themselves into the net, and clung to the meshes, gazing at the abyss. SENTENCE_END',\n",
              " 'SENTENCE_START the delicate sensibility of balloons is well known. SENTENCE_END',\n",
              " 'SENTENCE_START it is sufficient to throw out the lightest article to produce a difference in its vertical position. SENTENCE_END',\n",
              " 'SENTENCE_START the apparatus in the air is like a balance of mathematical precision. SENTENCE_END',\n",
              " 'SENTENCE_START it can be thus easily understood that when it is lightened of any considerable weight its movement will be impetuous and sudden. SENTENCE_END',\n",
              " 'SENTENCE_START so it happened on this occasion. SENTENCE_END',\n",
              " 'SENTENCE_START but after being suspended for an instant aloft, the balloon began to redescend, the gas escaping by the rent which it was impossible to repair. SENTENCE_END',\n",
              " 'SENTENCE_START the men had done all that men could do. SENTENCE_END',\n",
              " 'SENTENCE_START no human efforts could save them now. SENTENCE_END',\n",
              " 'SENTENCE_START they must trust to the mercy of him who rules the elements. SENTENCE_END',\n",
              " 'SENTENCE_START at four o’clock the balloon was only 500 feet above the surface of the water. SENTENCE_END',\n",
              " 'SENTENCE_START a loud barking was heard. SENTENCE_END',\n",
              " 'SENTENCE_START a dog accompanied the voyagers, and was held pressed close to his master in the meshes of the net. SENTENCE_END',\n",
              " 'SENTENCE_START “top has seen something,” cried one of the men. SENTENCE_END',\n",
              " 'SENTENCE_START then immediately a loud voice shouted,-- SENTENCE_END']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK5FXw_1Ow2I",
        "colab_type": "text"
      },
      "source": [
        "### The below code is used for part a). It concatenates every two consecutive sentences together to approximately double the sentence length that is being fed into our network for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOie6Rhj4d00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0\n",
        "sent = \"\"\n",
        "token_sent_double = []\n",
        "for sentence in sentence_list:\n",
        "  sent += sentence # set sent to two consecutive sentences\n",
        "  sent = sent.replace(' SENTENCE_ENDSENTENCE_START ', ' ') # need to get rid of start and end in middle after concatenating\n",
        "\n",
        "  count += 1\n",
        "  if count % 2 == 0:\n",
        "    token_sent_double.append(sent) # append to list\n",
        "    sent = \"\"\n",
        "    count = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEiX4_bvPe5t",
        "colab_type": "text"
      },
      "source": [
        "### The below code is used for part b). It halves every sentence to halve the sentence length that is being fed into our network for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh2aYjAD8L_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_sent_half = []\n",
        "for sentence in sentence_list:\n",
        "  sentence = nltk.word_tokenize(sentence) # tokenize into list of separate words\n",
        "  first_half, second_half = (sentence[:round(len(sentence)/2)]), (sentence[round(len(sentence)/2):]) # split string into first and second halves\n",
        "  first_half.append('SENTENCE_END') # add sentence end to first half at end\n",
        "  second_half.insert(0, 'SENTENCE_START') # add sentence start to second half at beginning\n",
        "  token_sent_half.append(\" \".join(first_half)) # must join before appending to rejoin into sentence string\n",
        "  token_sent_half.append(\" \".join(second_half))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkuv2UMCP7NJ",
        "colab_type": "text"
      },
      "source": [
        "### Get tokenized sentence lists for each separate part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SPTDsgJ1Zaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for first part\n",
        "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentence_list]\n",
        "\n",
        "# for second part with double length sentences\n",
        "tokenized_sentences_double = [nltk.word_tokenize(sent) for sent in token_sent_double]\n",
        "\n",
        "# for third part with half length sentences\n",
        "tokenized_sentences_half = [nltk.word_tokenize(sent) for sent in token_sent_half]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CH6DIts1Zas",
        "colab_type": "code",
        "outputId": "adc23139-9efa-4306-cdbc-c2cd30b2c9bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print((str(len(tokenized_sentences))) + \" tokenized sentences for first part.\")\n",
        "\n",
        "# Note how number of sentences in double the length is half the number compared to original\n",
        "print((str(len(tokenized_sentences_double))) + \" tokenized sentences for second part.\")\n",
        "\n",
        "# Note how number of sentences in half the length is double the number compared to original\n",
        "print((str(len(tokenized_sentences_half))) + \" tokenized sentences for third part.\")"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25324 tokenized sentences for first part.\n",
            "12662 tokenized sentences for second part.\n",
            "50648 tokenized sentences for third part.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh4jq3uA1Zau",
        "colab_type": "code",
        "outputId": "f6100c22-69eb-4f76-88f8-dd6ea936cc47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(tokenized_sentences[6])\n",
        "\n",
        "print(tokenized_sentences_double[6])\n",
        "\n",
        "print(tokenized_sentences_half[6])"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SENTENCE_START', 'on', 'the', 'contrary.', '”', '“', 'are', 'we', 'descending', '?', '”', '“', 'worse', 'than', 'that', ',', 'captain', '!', 'SENTENCE_END']\n",
            "['SENTENCE_START', 'such', 'were', 'the', 'loud', 'and', 'startling', 'words', 'which', 'resounded', 'through', 'the', 'air', ',', 'above', 'the', 'vast', 'watery', 'desert', 'of', 'the', 'pacific', ',', 'about', 'four', 'o', '’', 'clock', 'in', 'the', 'evening', 'of', 'the', '23rd', 'of', 'march', ',', '1865.', 'few', 'can', 'possibly', 'have', 'forgotten', 'the', 'terrible', 'storm', 'from', 'the', 'northeast', ',', 'in', 'the', 'middle', 'of', 'the', 'equinox', 'of', 'that', 'year', '.', 'SENTENCE_END']\n",
            "['SENTENCE_START', 'part', '1', '--', 'SENTENCE_END']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZXUPEw_1Zav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "\n",
        "word_freq_double = nltk.FreqDist(itertools.chain(*tokenized_sentences_double))\n",
        "\n",
        "word_freq_half = nltk.FreqDist(itertools.chain(*tokenized_sentences_half))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV6w3a8R1Zay",
        "colab_type": "code",
        "outputId": "ee78e71b-ca99-4b7a-e00d-ea804840eb77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"Found %d unique words tokens in tokenized_sentences.\" % len(word_freq.items()))\n",
        "\n",
        "print(\"Found %d unique words tokens in tokenized_sentences_double.\" % len(word_freq_double.items()))\n",
        "\n",
        "print(\"Found %d unique words tokens in tokenized_sentences_half.\" % len(word_freq_half.items()))"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 20063 unique words tokens in tokenized_sentences.\n",
            "Found 20064 unique words tokens in tokenized_sentences_double.\n",
            "Found 19206 unique words tokens in tokenized_sentences_half.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scSKD6kG1Za0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_freq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxlowFjdTfol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_freq_double"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLKPIuhkTf8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_freq_half"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfPIJ9B_1Za2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_1 = word_freq.most_common(vocabulary_size-1) # for original \n",
        "vocab_2 = word_freq_double.most_common(vocabulary_size-1) # for double\n",
        "vocab_3 = word_freq_half.most_common(vocabulary_size-1) # for half\n",
        "\n",
        "# index to word and word to index for original\n",
        "index_to_word_1 = [x[0] for x in vocab_1]\n",
        "index_to_word_1.append(unknown_token)\n",
        "word_to_index_1 = dict([(w,i) for i,w in enumerate(index_to_word_1)])\n",
        "\n",
        "# index to word and word to index for double\n",
        "index_to_word_2 = [x[0] for x in vocab_2]\n",
        "index_to_word_2.append(unknown_token)\n",
        "word_to_index_2 = dict([(w,i) for i,w in enumerate(index_to_word_2)])\n",
        "\n",
        "# index to word and word to index for half\n",
        "index_to_word_3 = [x[0] for x in vocab_3]\n",
        "index_to_word_3.append(unknown_token)\n",
        "word_to_index_3 = dict([(w,i) for i,w in enumerate(index_to_word_3)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnsM_OTD1Za4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FblULskXUaIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP1Vp1kwUafE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index_3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wep2Gpvm1Za6",
        "colab_type": "code",
        "outputId": "253885e5-7042-462f-8ad2-dec915b512df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
        "print(\"The least frequent word in our original vocabulary is '%s' and appeared %d times.\" % (vocab_1[-1][0], vocab_1[-1][1]))\n",
        "print(\"The least frequent word in our double vocabulary is '%s' and appeared %d times.\" % (vocab_2[-1][0], vocab_2[-1][1]))\n",
        "print(\"The least frequent word in our half vocabulary is '%s' and appeared %d times.\" % (vocab_3[-1][0], vocab_3[-1][1]))\n",
        "#word_freq.most_common(10)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using vocabulary size 8000.\n",
            "The least frequent word in our original vocabulary is 'sober' and appeared 3 times.\n",
            "The least frequent word in our double vocabulary is 'sober' and appeared 3 times.\n",
            "The least frequent word in our half vocabulary is 'boatmen' and appeared 3 times.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjzsUBsM1Za7",
        "colab_type": "code",
        "outputId": "e6502869-83e1-4afb-f4f1-f8d1e2280f07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(index_to_word_1[7999])\n",
        "print(index_to_word_2[7999])\n",
        "print(index_to_word_3[7999])"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UNKNOWN\n",
            "UNKNOWN\n",
            "UNKNOWN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG2aiCJV1Za9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replace all words not in vocabulary with the unknown token ORIGINAL\n",
        "for i, sentence in enumerate(tokenized_sentences):\n",
        "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sentence]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfXsnu55VkUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replace all words not in vocabulary with the unknown token DOUBLE\n",
        "for i, sentence in enumerate(tokenized_sentences_double):\n",
        "    tokenized_sentences_double[i] = [w if w in word_to_index else unknown_token for w in sentence]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaxv1DYSVop8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replace all words not in vocabulary with the unknown token HALF\n",
        "for i, sentence in enumerate(tokenized_sentences_half):\n",
        "    tokenized_sentences_half[i] = [w if w in word_to_index else unknown_token for w in sentence]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC5zk0AU1Za_",
        "colab_type": "code",
        "outputId": "b4a67249-0d36-4108-9bc6-fa28e8566375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(\"\\nExample sentence: '%s'\" % sentence_list[3])\n",
        "print(\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[3])"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Example sentence: 'SENTENCE_START  part 1--dropped from the clouds SENTENCE_END'\n",
            "\n",
            "Example sentence after Pre-processing: '['SENTENCE_START', 'part', '1', '--', 'dropped', 'from', 'the', 'clouds', 'SENTENCE_END']'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8ix42eqWQ8n",
        "colab_type": "text"
      },
      "source": [
        "### Create training data for each part of Task 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4prklGP1ZbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the training data for original\n",
        "XTrain = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
        "YTrain = np.asarray([[word_to_index[w] for w in sent [1:]] for sent in tokenized_sentences])\n",
        "\n",
        "# Create the training data for double\n",
        "XTrain_double = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences_double])\n",
        "YTrain_double = np.asarray([[word_to_index[w] for w in sent [1:]] for sent in tokenized_sentences_double])\n",
        "\n",
        "# Create the training data for half\n",
        "XTrain_half = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences_half])\n",
        "YTrain_half = np.asarray([[word_to_index[w] for w in sent [1:]] for sent in tokenized_sentences_half])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id9yC6gW1ZbD",
        "colab_type": "code",
        "outputId": "ecf3149b-fa90-49c0-b774-f78f67b4c66a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "XTrain"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([2, 1, 843, 77]), list([2, 26, 4825, 1537]), list([2, 7999]),\n",
              "       ...,\n",
              "       list([2, 299, 16, 68, 13, 1161, 343, 541, 5, 18, 7999, 590, 38]),\n",
              "       list([2, 347, 6, 15, 313, 146, 7006, 419, 1495, 9, 1, 1262, 5, 7999, 10, 85, 104, 5657, 1, 7999, 425, 38, 11, 7999, 269, 87, 5, 46, 1703, 40, 84, 4347, 1, 179, 6, 392, 4]),\n",
              "       list([2, 63, 106, 7, 5246])], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b7812cba-5f98-4a03-9ef0-1c5d9d945347",
        "id": "p6ua76-rWebi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "XTrain_double"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([2, 1, 843, 77, 26, 4825, 1537]),\n",
              "       list([2, 7999, 177, 1303, 69, 1891, 35, 1, 751]),\n",
              "       list([2, 263, 1303, 10, 71, 27, 579, 173, 38, 11, 10, 44, 4]), ...,\n",
              "       list([2, 185, 1, 7999, 724, 200, 0, 7, 1, 4779, 1355, 25, 3516, 902, 5, 1, 339, 24, 61, 25, 6117, 37, 569, 0, 14, 16, 39, 215, 4291, 4]),\n",
              "       list([2, 1675, 16, 68, 13, 7999, 14, 262, 38, 299, 16, 68, 13, 1161, 343, 541, 5, 18, 7999, 590, 38]),\n",
              "       list([2, 347, 6, 15, 313, 146, 7006, 419, 1495, 9, 1, 1262, 5, 7999, 10, 85, 104, 5657, 1, 7999, 425, 38, 11, 7999, 269, 87, 5, 46, 1703, 40, 84, 4347, 1, 179, 6, 392, 4, 63, 106, 7, 5246])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "01077758-ea8f-43c1-e10f-0fec2e36fb32",
        "id": "nEzHqCFQWfld",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "XTrain_half"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([2, 1]), list([2, 843, 77]), list([2, 26]), ...,\n",
              "       list([2, 7999, 425, 38, 11, 7999, 269, 87, 5, 46, 1703, 40, 84, 4347, 1, 179, 6, 392, 4]),\n",
              "       list([2, 63, 106]), list([2, 7, 5246])], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrqKSdUA1ZbG",
        "colab_type": "code",
        "outputId": "2a191aab-b127-4dbe-af62-09a5d09b45d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "YTrain"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 843, 77, 3]), list([26, 4825, 1537, 3]), list([7999, 3]),\n",
              "       ...,\n",
              "       list([299, 16, 68, 13, 1161, 343, 541, 5, 18, 7999, 590, 38, 3]),\n",
              "       list([347, 6, 15, 313, 146, 7006, 419, 1495, 9, 1, 1262, 5, 7999, 10, 85, 104, 5657, 1, 7999, 425, 38, 11, 7999, 269, 87, 5, 46, 1703, 40, 84, 4347, 1, 179, 6, 392, 4, 3]),\n",
              "       list([63, 106, 7, 5246, 3])], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ba94d4d9-8637-433c-938d-5e0f61dcaae2",
        "id": "YhPGiChCWlgn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "YTrain_double"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 843, 77, 26, 4825, 1537, 3]),\n",
              "       list([7999, 177, 1303, 69, 1891, 35, 1, 751, 3]),\n",
              "       list([263, 1303, 10, 71, 27, 579, 173, 38, 11, 10, 44, 4, 3]), ...,\n",
              "       list([185, 1, 7999, 724, 200, 0, 7, 1, 4779, 1355, 25, 3516, 902, 5, 1, 339, 24, 61, 25, 6117, 37, 569, 0, 14, 16, 39, 215, 4291, 4, 3]),\n",
              "       list([1675, 16, 68, 13, 7999, 14, 262, 38, 299, 16, 68, 13, 1161, 343, 541, 5, 18, 7999, 590, 38, 3]),\n",
              "       list([347, 6, 15, 313, 146, 7006, 419, 1495, 9, 1, 1262, 5, 7999, 10, 85, 104, 5657, 1, 7999, 425, 38, 11, 7999, 269, 87, 5, 46, 1703, 40, 84, 4347, 1, 179, 6, 392, 4, 63, 106, 7, 5246, 3])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "346cb70c-d6c9-4650-83e1-d29f4bba0b73",
        "id": "MdMw6EUIWlwp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "YTrain_half"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 3]), list([843, 77, 3]), list([26, 3]), ...,\n",
              "       list([7999, 425, 38, 11, 7999, 269, 87, 5, 46, 1703, 40, 84, 4347, 1, 179, 6, 392, 4, 3]),\n",
              "       list([63, 106, 3]), list([7, 5246, 3])], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff-Hx6LLWu_5",
        "colab_type": "text"
      },
      "source": [
        "### Show 5 random sentences from XTrain and YTrain, to show how YTrain is just XTrain shifted once to the left"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwqbQ3BC1ZbI",
        "colab_type": "code",
        "outputId": "fda90386-16be-4b91-b5c2-178ae9dd0914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "import random\n",
        "\n",
        "for i in range(5):\n",
        "    idx = random.randint(0, len(XTrain))\n",
        "    x_example, y_example = XTrain[idx], YTrain[idx]\n",
        "    print(\"Sample id %d\\n========================\\n\" % idx)\n",
        "    print(\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
        "    print(\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample id 9976\n",
            "========================\n",
            "\n",
            "x:\n",
            "SENTENCE_START “ the subterranean fires have therefore been UNKNOWN for ten weeks , ” resumed gideon spilett , “ and it is not to be wondered at that they now break out with such violence ! ”\n",
            "[2, 10, 1, 1026, 1690, 40, 229, 57, 7999, 32, 343, 2348, 0, 11, 890, 180, 123, 0, 10, 7, 14, 37, 28, 6, 31, 3028, 23, 15, 36, 84, 1221, 87, 22, 135, 1207, 24, 11]\n",
            "\n",
            "y:\n",
            "“ the subterranean fires have therefore been UNKNOWN for ten weeks , ” resumed gideon spilett , “ and it is not to be wondered at that they now break out with such violence ! ” SENTENCE_END\n",
            "[10, 1, 1026, 1690, 40, 229, 57, 7999, 32, 343, 2348, 0, 11, 890, 180, 123, 0, 10, 7, 14, 37, 28, 6, 31, 3028, 23, 15, 36, 84, 1221, 87, 22, 135, 1207, 24, 11, 3]\n",
            "Sample id 11013\n",
            "========================\n",
            "\n",
            "x:\n",
            "SENTENCE_START `` on tuesday morning at seven o'clock be on board , '' said m. bjarne , UNKNOWN us our UNKNOWN .\n",
            "[2, 50, 19, 5895, 344, 23, 638, 1114, 31, 19, 355, 0, 49, 60, 1815, 6771, 0, 7999, 72, 48, 7999, 4]\n",
            "\n",
            "y:\n",
            "`` on tuesday morning at seven o'clock be on board , '' said m. bjarne , UNKNOWN us our UNKNOWN . SENTENCE_END\n",
            "[50, 19, 5895, 344, 23, 638, 1114, 31, 19, 355, 0, 49, 60, 1815, 6771, 0, 7999, 72, 48, 7999, 4, 3]\n",
            "Sample id 15516\n",
            "========================\n",
            "\n",
            "x:\n",
            "SENTENCE_START as far as we had hitherto gone , facts had proved the theories of davy and of UNKNOWN to be correct .\n",
            "[2, 29, 216, 29, 27, 17, 2875, 467, 0, 2588, 17, 1394, 1, 2722, 5, 4294, 7, 5, 7999, 6, 31, 1691, 4]\n",
            "\n",
            "y:\n",
            "as far as we had hitherto gone , facts had proved the theories of davy and of UNKNOWN to be correct . SENTENCE_END\n",
            "[29, 216, 29, 27, 17, 2875, 467, 0, 2588, 17, 1394, 1, 2722, 5, 4294, 7, 5, 7999, 6, 31, 1691, 4, 3]\n",
            "Sample id 23221\n",
            "========================\n",
            "\n",
            "x:\n",
            "SENTENCE_START without it , no sights were possible .\n",
            "[2, 95, 14, 0, 44, 2496, 34, 323, 4]\n",
            "\n",
            "y:\n",
            "without it , no sights were possible . SENTENCE_END\n",
            "[95, 14, 0, 44, 2496, 34, 323, 4, 3]\n",
            "Sample id 18685\n",
            "========================\n",
            "\n",
            "x:\n",
            "SENTENCE_START an underwater forest\n",
            "[2, 45, 449, 395]\n",
            "\n",
            "y:\n",
            "an underwater forest SENTENCE_END\n",
            "[45, 449, 395, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMNoHwkkW_Q2",
        "colab_type": "text"
      },
      "source": [
        "### Create the Vanilla RNN Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKzyZFqT1ZbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNVanilla:\n",
        "\n",
        "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
        "\n",
        "        # Assign instance variables\n",
        "        self.word_dim = word_dim # size of the vocabulary\n",
        "        self.hidden_dim = hidden_dim # size of hidden layer\n",
        "        self.bptt_truncate = bptt_truncate\n",
        "\n",
        "        # Randomly initialize the network parameters\n",
        "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
        "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
        "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j8mHHcD1ZbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "  ''' softmax function '''\n",
        "  xt = np.exp(x - np.max(x))\n",
        "  return xt / np.sum(xt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94f3Wm741ZbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_propagation(self, x):\n",
        "    # Total number of time steps\n",
        "    T = len(x)\n",
        "\n",
        "    # During forward propagation we save all hidden states in s because we need them later.\n",
        "\n",
        "    # We add one additional element for the initial hidden, which we set to 0\n",
        "    s = np.zeros((T + 1, self.hidden_dim))\n",
        "    s[-1] = np.zeros(self.hidden_dim)\n",
        "\n",
        "    # The outputs at each time step, Again, we save them for later\n",
        "    o = np.zeros((T, self.word_dim))\n",
        "\n",
        "    # For each time step...\n",
        "    for t in np.arange(T):\n",
        "        # Note that we are indexing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
        "        s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))\n",
        "        o[t] = softmax(self.V.dot(s[t]))\n",
        "    return [o, s] # We not only return the calculated outputs, but also the hidden states.\n",
        "                  # We will use them later to calculate the gradients\n",
        "\n",
        "# Now make it a member of the class RNNVanilla\n",
        "RNNVanilla.forward_propagation = forward_propagation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_6E-NBc1ZbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(self, x):\n",
        "    # Perform forward propagation and return index of the highest score\n",
        "    o, s = self.forward_propagation(x)\n",
        "    return np.argmax(o, axis=1)\n",
        "\n",
        "# Now make it a member of the class RNNVanilla\n",
        "RNNVanilla.predict = predict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAo1g6Zh1ZbS",
        "colab_type": "code",
        "outputId": "bb8cf2ae-3ca1-4ac3-a9b4-e5219177e6e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "print(\"Length of XTrain[10] is %d\" % (len(XTrain[10])))\n",
        "np.random.seed(100)\n",
        "\n",
        "model = RNNVanilla(vocabulary_size)\n",
        "o, s = model.forward_propagation(XTrain[10])\n",
        "print(o.shape)\n",
        "print(o)\n",
        "# For each word in the sentence, the model made 8000 predictions representing probabilities of the next word\n",
        "# Note that because we initialized U, V, W to random values these predictions are completely random right now"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of XTrain[10] is 19\n",
            "(19, 8000)\n",
            "[[0.0001246  0.00012553 0.00012589 ... 0.00012485 0.00012517 0.00012529]\n",
            " [0.00012536 0.00012431 0.00012473 ... 0.00012514 0.00012464 0.00012557]\n",
            " [0.00012478 0.00012452 0.00012558 ... 0.00012454 0.00012574 0.00012446]\n",
            " ...\n",
            " [0.00012477 0.00012464 0.00012533 ... 0.00012504 0.00012468 0.00012499]\n",
            " [0.00012518 0.00012559 0.00012501 ... 0.00012528 0.00012452 0.0001252 ]\n",
            " [0.00012431 0.00012588 0.00012578 ... 0.00012629 0.00012445 0.00012479]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mWlF0aB_1ZbU",
        "colab_type": "code",
        "outputId": "27732df7-f13e-4e4b-87a2-55b53257d205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# The following gives the indices of the highest probability predictions for each word:\n",
        "predictions = model.predict(XTrain[10])\n",
        "print(predictions.shape)\n",
        "print(predictions)\n",
        "print(\"index_to_word>\")\n",
        "print('%s'%\" \".join([index_to_word[x] for x in predictions]))"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19,)\n",
            "[1386 2542 1193 7993 3728 6162 6969 2924 6734 5804 7288 3712 6787 4191\n",
            "   67 1409 6734 6695 5419]\n",
            "index_to_word>\n",
            "expect liked worth 1219 & anywhere expert navigation mounting weakened lion outline veil loudly him king mounting occurring recollect\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01nMP49T1ZbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_total_loss(self, x, y):\n",
        "    L = 0\n",
        "\n",
        "    # For each sentence\n",
        "    for i in np.arange(len(y)):\n",
        "        o, s = self.forward_propagation(x[i])\n",
        "\n",
        "        # We only care about our prediction of the \"correct\" words\n",
        "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
        "\n",
        "        # Add to the loss based on how off we were\n",
        "        L += -1 * sum(np.log(correct_word_predictions))\n",
        "    return L\n",
        "\n",
        "def calculate_loss(self, x, y):\n",
        "    # Divide the total loss by the number of training examples\n",
        "    N = sum((len(y_i) for y_i in y))\n",
        "    return self.calculate_total_loss(x,y)/N\n",
        "    \n",
        "RNNVanilla.calculate_total_loss = calculate_total_loss\n",
        "RNNVanilla.calculate_loss = calculate_loss\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t-jaM5J1ZbX",
        "colab_type": "code",
        "outputId": "6700fd26-2e98-4ac3-d262-01034f620073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.log(vocabulary_size)\n"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.987196820661973"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcLnGcvd1Zba",
        "colab_type": "code",
        "outputId": "bcda8038-1679-425a-8d9f-cc4190e117aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "XTrain[:100].shape"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25A8C9PN1Zbd",
        "colab_type": "code",
        "outputId": "189f531f-ef04-452e-ddf3-7e391a514a7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "XTrain.shape"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25324,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caUVD8GF1Zbf",
        "colab_type": "code",
        "outputId": "3548d6aa-bf62-4cd8-e84a-a4231118c01c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Limit to 1000 examples to save time\n",
        "print(\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
        "print(\"Actual loss: %f\" % model.calculate_loss(XTrain[:1000], YTrain[:1000]))\n"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected Loss for random predictions: 8.987197\n",
            "Actual loss: 8.987239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O8SLcZh1Zbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bptt(self, x, y):\n",
        "    T = len(y)\n",
        "    # Perform forward propagation\n",
        "    o, s = self.forward_propagation(x)\n",
        "    # We accumulate the gradients in these variables\n",
        "    dLdU = np.zeros(self.U.shape)\n",
        "    dLdV = np.zeros(self.V.shape)\n",
        "    dLdW = np.zeros(self.W.shape)\n",
        "    delta_o = o\n",
        "    delta_o[np.arange(len(y)), y] -= 1.\n",
        "    # For each output backwards...\n",
        "    for t in np.arange(T)[::-1]:\n",
        "        dLdV += np.outer(delta_o[t], s[t].T)\n",
        "\n",
        "        # Initial delta calculation\n",
        "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
        "\n",
        "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
        "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
        "\n",
        "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
        "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
        "            dLdU[:,x[bptt_step]] += delta_t\n",
        "\n",
        "            # Update delta for next step\n",
        "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
        "    return [dLdU, dLdV, dLdW]\n",
        "    \n",
        "\n",
        "RNNVanilla.bptt = bptt       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxbBTvwU1Zbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
        "    # Calculate the gradients using backpropagation. We want to check if these are correct.\n",
        "    bptt_gradients = self.bptt(x, y)\n",
        "\n",
        "    # List of all parameters we want to check.\n",
        "    model_parameters = ['U', 'V', 'W']\n",
        "\n",
        "    # Gradient check for each parameter\n",
        "    for pidx, pname in enumerate(model_parameters):\n",
        "        # Get the actual parameter value from the mode, e.g. model.W\n",
        "        parameter = operator.attrgetter(pname)(self)\n",
        "        print(\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))\n",
        "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
        "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
        "        while not it.finished:\n",
        "            ix = it.multi_index\n",
        "            # Save the original value so we can reset it later\n",
        "            original_value = parameter[ix]\n",
        "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
        "            parameter[ix] = original_value + h\n",
        "            gradplus = self.calculate_total_loss([x], [y])\n",
        "            parameter[ix] = original_value - h\n",
        "            gradminus = self.calculate_total_loss([x], [y])\n",
        "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
        "            # Reset parameter to original value\n",
        "            parameter[ix] = original_value\n",
        "            # The gradient for this parameter calculated using backpropagation\n",
        "            backprop_gradient = bptt_gradients[pidx][ix]\n",
        "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
        "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
        "            # If the error is to large fail the gradient check\n",
        "            if relative_error > error_threshold:\n",
        "                print (\"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
        "                print (\"+h Loss: %f\" % gradplus)\n",
        "                print (\"-h Loss: %f\" % gradminus)\n",
        "                print (\"Estimated_gradient: %f\" % estimated_gradient)\n",
        "                print (\"Backpropagation gradient: %f\" % backprop_gradient)\n",
        "                print (\"Relative Error: %f\" % relative_error)\n",
        "                return\n",
        "            it.iternext()\n",
        "        print (\"Gradient check for parameter %s passed.\" % (pname))\n",
        "\n",
        "RNNVanilla.gradient_check = gradient_check"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MDowIt61Zbk",
        "colab_type": "code",
        "outputId": "3acd052b-2ed6-4981-b0c0-8020583ab98f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
        "grad_check_vocab_size = 100\n",
        "np.random.seed(10)\n",
        "model = RNNVanilla(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
        "model.gradient_check([0,1,2,3], [1,2,3,4])"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing gradient check for parameter U with size 1000.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient check for parameter U passed.\n",
            "Performing gradient check for parameter V with size 1000.\n",
            "Gradient check for parameter V passed.\n",
            "Performing gradient check for parameter W with size 100.\n",
            "Gradient check for parameter W passed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mssIH5A_1Zbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Performs one step of SGD.\n",
        "def numpy_sdg_step(self, x, y, learning_rate):\n",
        "    # Calculate the gradients\n",
        "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
        "    # Change parameters according to gradients and learning rate\n",
        "    self.U -= learning_rate * dLdU\n",
        "    self.V -= learning_rate * dLdV\n",
        "    self.W -= learning_rate * dLdW\n",
        "    \n",
        "RNNVanilla.sgd_step = numpy_sdg_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0sVpF8y1Zbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Outer SGD Loop\n",
        "# - model: The RNN model instance\n",
        "# - X_train: The training data set\n",
        "# - y_train: The training data labels\n",
        "# - learning_rate: Initial learning rate for SGD\n",
        "# - nepoch: Number of times to iterate through the complete dataset\n",
        "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
        "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
        "    # We keep track of the losses so we can plot them later\n",
        "    losses = []\n",
        "    num_examples_seen = 0\n",
        "    for epoch in range(nepoch):\n",
        "        # Optionally evaluate the loss\n",
        "        if (epoch % evaluate_loss_after == 0):\n",
        "            loss = model.calculate_loss(X_train, y_train)\n",
        "            losses.append((num_examples_seen, loss))\n",
        "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            print (\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
        "            # Adjust the learning rate if loss increases\n",
        "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
        "                learning_rate = learning_rate * 0.5 \n",
        "                print (\"Setting learning rate to %f\" % learning_rate)\n",
        "            sys.stdout.flush()\n",
        "        # For each training example...\n",
        "        for i in range(len(y_train)):\n",
        "            # One SGD step\n",
        "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
        "            num_examples_seen += 1\n",
        "    return losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2XSY4X11Zbp",
        "colab_type": "code",
        "outputId": "72a6d3c0-4dcb-4c9f-a601-e116c595ebca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.random.seed(10)\n",
        "model = RNNVanilla(vocabulary_size)\n",
        "%timeit model.sgd_step(XTrain[10], YTrain[10], learning_rate=0.005)"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 3: 80 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C582rxXUEAhq",
        "colab_type": "text"
      },
      "source": [
        "# 5 Breakpoints\n",
        "\n",
        "## We train below at 5 different breakpoints: 6, 12, 18, 24 and 30 epochs to show how well our network learns over time.\n",
        "\n",
        "## We feed our network just a subset of our data (0-99) of XTrain and YTrain.\n",
        "\n",
        "## After each breakpoint trained, we generate 10 sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxVlN6imFh1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_sentence(model):\n",
        "    # We start the sentence with the start token\n",
        "    new_sentence = [word_to_index[sentence_start_token]]\n",
        "    \n",
        "    # Repeat until we get an end token\n",
        "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
        "        next_word_probs = model.forward_propagation(new_sentence)\n",
        "        sampled_word = word_to_index[unknown_token]\n",
        "        # We don't want to sample unknown words\n",
        "        while sampled_word == word_to_index[unknown_token]:\n",
        "            samples = np.random.multinomial(1, next_word_probs[0][-1])\n",
        "            sampled_word = np.argmax(samples)\n",
        "        new_sentence.append(sampled_word)\n",
        "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
        "    return sentence_str"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5GL03VIcBlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_sentences_with_model(model, num_sentences, senten_min_length, model_num):\n",
        "  if model_num != 0:\n",
        "    # Generate 10 sentences for each trained model to compare \n",
        "    print (\"Generating sentences for model \" + str(model_num) + \" ...\")\n",
        "  for i in range(num_sentences):\n",
        "    sent = []\n",
        "      # We want long sentences, not sentences with one or two words\n",
        "    while len(sent) < senten_min_length:\n",
        "      sent = generate_sentence(model)\n",
        "    print (\"Sentence # %d of length %d\" % ((i+1),  len(sent)))\n",
        "    print (\" \".join(sent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGOy0R0xcM30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_sentences = 10\n",
        "senten_min_length = 7\n",
        "breakpoint_size = 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfay6aaq1Zbs",
        "colab_type": "code",
        "outputId": "85940301-1cd6-49e7-f6bc-3220e224759a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "np.random.seed(10)\n",
        "# Train on a small subset of the data to see what happens for first 6 epochs\n",
        "model = RNNVanilla(vocabulary_size)\n",
        "losses = train_with_sgd(model, XTrain[:100], YTrain[:100], nepoch=breakpoint_size, evaluate_loss_after=1)"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-30 02:34:23: Loss after num_examples_seen=0 epoch=0: 8.987121\n",
            "2019-10-30 02:34:35: Loss after num_examples_seen=100 epoch=1: 8.973798\n",
            "2019-10-30 02:34:46: Loss after num_examples_seen=200 epoch=2: 8.953741\n",
            "2019-10-30 02:34:56: Loss after num_examples_seen=300 epoch=3: 8.909435\n",
            "2019-10-30 02:35:06: Loss after num_examples_seen=400 epoch=4: 6.694111\n",
            "2019-10-30 02:35:17: Loss after num_examples_seen=500 epoch=5: 6.147468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pThOWYsLcT7R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "24902b68-5e0d-41e0-cfce-5ff618d3981e"
      },
      "source": [
        "model_num = 1\n",
        "generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating sentences for model 1 ...\n",
            "Sentence # 1 of length 13\n",
            "newly burrows through only firing any of empty and like they of .\n",
            "Sentence # 2 of length 28\n",
            "convoy encircled shout tanks to ? and easily . sea impetuous to balloon that stepped intended was is the was pigeons the dioxide . in invitation they car\n",
            "Sentence # 3 of length 12\n",
            "principally confound . the a after been if the prodigious remedy only\n",
            "Sentence # 4 of length 36\n",
            "food feet energetic honesty the be balloon the it them industry a , of joints a levers . the snowy the of to luggage . was bush discovering free snatched , were of be which walls\n",
            "Sentence # 5 of length 15\n",
            "venturing putting had studied clue smallest of flushed to sustained crouching the . to is\n",
            "Sentence # 6 of length 11\n",
            "tolerable efforts in the “ in less leg feet pomoutous love\n",
            "Sentence # 7 of length 35\n",
            "van difference luggage . a , of into do america 2,000 visible the witnessed nemo. producing appropriate the the trust accomplished ax by . a faded thank only foods inside , , a hastened lively\n",
            "Sentence # 8 of length 9\n",
            "quantities unrolled their rival dress shifted to reader of\n",
            "Sentence # 9 of length 8\n",
            "obedient glimpsed clouds descend net to of .\n",
            "Sentence # 10 of length 15\n",
            "beneath impede crowbars usually and of nautical still , nuts be pipes is the innocent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xyH0TriVE6Z0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "60cb8a88-1a5c-44ed-d283-11a0d9c1f00c"
      },
      "source": [
        "np.random.seed(10)\n",
        "# Next 6 epochs\n",
        "losses_temp = train_with_sgd(model, XTrain[:100], YTrain[:100], nepoch=breakpoint_size, evaluate_loss_after=1)\n",
        "for loss in losses_temp:\n",
        "  losses.append(loss)"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-30 02:35:29: Loss after num_examples_seen=0 epoch=0: 5.899774\n",
            "2019-10-30 02:35:39: Loss after num_examples_seen=100 epoch=1: 5.732092\n",
            "2019-10-30 02:35:49: Loss after num_examples_seen=200 epoch=2: 5.607180\n",
            "2019-10-30 02:36:00: Loss after num_examples_seen=300 epoch=3: 5.502346\n",
            "2019-10-30 02:36:11: Loss after num_examples_seen=400 epoch=4: 5.413354\n",
            "2019-10-30 02:36:23: Loss after num_examples_seen=500 epoch=5: 5.338649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gKSuFgNDcfDe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "1ab3282c-ef05-4f6e-b8cc-563481c09787"
      },
      "source": [
        "model_num = 2\n",
        "generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating sentences for model 2 ...\n",
            "Sentence # 1 of length 35\n",
            "linen their only as had the , the triumph to weather last 12th a feet interminable was gas the ? long withdraw hoarse could the 5 , the counting distance who heard they which .\n",
            "Sentence # 2 of length 10\n",
            "the they , theirs maelstrom from contrived radius hold beating\n",
            "Sentence # 3 of length 18\n",
            "buildings bits it of did s. miles manage talents could beneath the to only only top , .\n",
            "Sentence # 4 of length 21\n",
            "energetic was , like more the rate loud of which out warn the ” overflowed “ hovered , that , .\n",
            "Sentence # 5 of length 16\n",
            "commotion planking the a men car miles northeast passed out men “ ways , to for\n",
            "Sentence # 6 of length 18\n",
            "began . not which only waited their might ” any voice was death even were more could .\n",
            "Sentence # 7 of length 24\n",
            "miraculous but on , they not is as labor gazing “ . numbered out their oval , chemist car movement it feet tempest with\n",
            "Sentence # 8 of length 11\n",
            "the it was possibility abyss was and and the winters .\n",
            "Sentence # 9 of length 21\n",
            "madman march turnips was , the dreadful , regions as delicate however be them lower forty , of could , occasion\n",
            "Sentence # 10 of length 8\n",
            "iron men the tolerable , 1219 most .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fV-vCMw-E7xq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "ef33932a-0b73-4e44-a305-b547ffba3245"
      },
      "source": [
        "np.random.seed(10)\n",
        "# Next 6 epochs\n",
        "losses_temp = train_with_sgd(model, XTrain[:100], YTrain[:100], nepoch=breakpoint_size, evaluate_loss_after=1)\n",
        "for loss in losses_temp:\n",
        "  losses.append(loss)"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-30 02:36:36: Loss after num_examples_seen=0 epoch=0: 5.272850\n",
            "2019-10-30 02:36:47: Loss after num_examples_seen=100 epoch=1: 5.212487\n",
            "2019-10-30 02:36:59: Loss after num_examples_seen=200 epoch=2: 5.157330\n",
            "2019-10-30 02:37:10: Loss after num_examples_seen=300 epoch=3: 5.108244\n",
            "2019-10-30 02:37:20: Loss after num_examples_seen=400 epoch=4: 5.063920\n",
            "2019-10-30 02:37:30: Loss after num_examples_seen=500 epoch=5: 5.023586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f--zYRowcfWq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "cd20fb6b-bc07-41a7-bb00-e6a42e32bbef"
      },
      "source": [
        "model_num = 3\n",
        "generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating sentences for model 3 ...\n",
            "Sentence # 1 of length 60\n",
            "it car ’ ” from only their the , inevitable to last whole in not waves not atmosphere layers sensible penny hoarse could the 5 , the counting distance who heard they which is , which two sole , while hoisted is tempest dialect voyagers call a be the net lower balloon maelstrom was slightest rent an must are .\n",
            "Sentence # 2 of length 19\n",
            "a admirably capitally solitary which a ocean even in above sea ? marksmen talents could beneath the to liked\n",
            "Sentence # 3 of length 8\n",
            "the well of half at the , of\n",
            "Sentence # 4 of length 12\n",
            "sort loud by out ship the its , hours hold surface .\n",
            "Sentence # 5 of length 30\n",
            "increasing maintaining lightened on the accompanied of the thanksgiving , which ropes the a of the cords of its as men since the dropped water , balloon the ” 500\n",
            "Sentence # 6 of length 11\n",
            "conseil. are , ship sink , to for , dropped which\n",
            "Sentence # 7 of length 10\n",
            "infernal was had on , they not watery man .\n",
            "Sentence # 8 of length 9\n",
            "cried moving gazing “ a before the course .\n",
            "Sentence # 9 of length 24\n",
            "white swung arrest beneath beneath it ” car movement it feet tempest with of it were ? of was furious sack longer can .\n",
            "Sentence # 10 of length 13\n",
            "generally escape also loud fear position four to ” if was which .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FqBmiOtHE8BY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "ad088c30-9093-4288-884b-10f993281a8b"
      },
      "source": [
        "np.random.seed(10)\n",
        "# Next 6 epochs\n",
        "losses_temp = train_with_sgd(model, XTrain[:100], YTrain[:100], nepoch=breakpoint_size, evaluate_loss_after=1)\n",
        "for loss in losses_temp:\n",
        "  losses.append(loss)"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-30 02:37:43: Loss after num_examples_seen=0 epoch=0: 4.986770\n",
            "2019-10-30 02:37:53: Loss after num_examples_seen=100 epoch=1: 4.952559\n",
            "2019-10-30 02:38:03: Loss after num_examples_seen=200 epoch=2: 4.919954\n",
            "2019-10-30 02:38:14: Loss after num_examples_seen=300 epoch=3: 4.888089\n",
            "2019-10-30 02:38:24: Loss after num_examples_seen=400 epoch=4: 4.856466\n",
            "2019-10-30 02:38:34: Loss after num_examples_seen=500 epoch=5: 4.825223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vhzywhUjcfie",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "42d73d17-832a-4cb6-da97-0822eaeeaa10"
      },
      "source": [
        "model_num = 4\n",
        "generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating sentences for model 4 ...\n",
            "Sentence # 1 of length 10\n",
            "it car ’ ” from only their the net .\n",
            "Sentence # 2 of length 20\n",
            "concealed is itself in not waves not energies to , they interminable was gas layers descending by 1 done .\n",
            "Sentence # 3 of length 14\n",
            "former urville o fell and only its a engine distance who heard been .\n",
            "Sentence # 4 of length 12\n",
            "laughing related whose on his be is convalescence 23rd the net .\n",
            "Sentence # 5 of length 13\n",
            "the soon being the gas to destitute northeast useful , the car .\n",
            "Sentence # 6 of length 15\n",
            "it , ocean was voice only appear less them the all of the cluster .\n",
            "Sentence # 7 of length 47\n",
            "the dawn , passed midst entrails talents the route was moreover sensible contained devastating which the fluid to in to a leveled a the enterprise northeast passed out men since the dropped of like , circles not artistic , to for , dropped which oval could .\n",
            "Sentence # 8 of length 13\n",
            "lions not watery man this , is -- mercy half is hurricane .\n",
            "Sentence # 9 of length 9\n",
            "lean in they , whose of moving rotation .\n",
            "Sentence # 10 of length 8\n",
            "stories swung arrest their published the a ,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KoGsJS-2E8Ij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "6f799ea2-bdcd-430d-d951-7efbac2de193"
      },
      "source": [
        "np.random.seed(10)\n",
        "# Next 6 epochs\n",
        "losses_temp = train_with_sgd(model, XTrain[:100], YTrain[:100], nepoch=breakpoint_size, evaluate_loss_after=1)\n",
        "for loss in losses_temp:\n",
        "  losses.append(loss)"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-30 02:38:48: Loss after num_examples_seen=0 epoch=0: 4.795049\n",
            "2019-10-30 02:39:00: Loss after num_examples_seen=100 epoch=1: 4.766487\n",
            "2019-10-30 02:39:12: Loss after num_examples_seen=200 epoch=2: 4.739924\n",
            "2019-10-30 02:39:23: Loss after num_examples_seen=300 epoch=3: 4.716038\n",
            "2019-10-30 02:39:33: Loss after num_examples_seen=400 epoch=4: 4.694276\n",
            "2019-10-30 02:39:44: Loss after num_examples_seen=500 epoch=5: 4.672641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mVPcE3Dycfwf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "5515c26b-f394-4dfd-b527-cba64fb1665c"
      },
      "source": [
        "model_num = 5\n",
        "generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating sentences for model 5 ...\n",
            "Sentence # 1 of length 10\n",
            "it car ’ ” from only their the net .\n",
            "Sentence # 2 of length 21\n",
            "concealed is itself in not waves of must sea to frightfully height a feet , the voyagers of a transit .\n",
            "Sentence # 3 of length 17\n",
            "“ was what with hoarse mercy loud the little , the counting distance who heard been .\n",
            "Sentence # 4 of length 7\n",
            "laughing descent a were the transit .\n",
            "Sentence # 5 of length 7\n",
            "the no was moderated voyagers call .\n",
            "Sentence # 6 of length 30\n",
            "scarcely was perish great aloft ? marksmen talents layers , working was , like more the balloon away loud by out ship the its , hours was a pressed .\n",
            "Sentence # 7 of length 8\n",
            "it voyagers rate at seem had still .\n",
            "Sentence # 8 of length 11\n",
            "difference vast in to without ’ to to the leveled .\n",
            "Sentence # 9 of length 28\n",
            "the started not immediately to the hatred of ” which many the dropped of like , downward also , the abyss which drowned the was , to for\n",
            "Sentence # 10 of length 16\n",
            "they a reach heart falling less only hovering to ship terrible those tried watery man .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LO3J_1Xs1Zbt",
        "colab_type": "code",
        "outputId": "e080a6b9-861c-463e-ec7f-1721c666a8bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# Print losses for final trained model\n",
        "for loss in losses:\n",
        "    print(\"Losses for final trained model: \" + str(loss))"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Losses for final trained model: (0, 8.98712113087443)\n",
            "Losses for final trained model: (100, 8.973797863513905)\n",
            "Losses for final trained model: (200, 8.953741236301367)\n",
            "Losses for final trained model: (300, 8.909434562512477)\n",
            "Losses for final trained model: (400, 6.6941105257575275)\n",
            "Losses for final trained model: (500, 6.147467515717594)\n",
            "Losses for final trained model: (0, 5.899773973421222)\n",
            "Losses for final trained model: (100, 5.732091640797683)\n",
            "Losses for final trained model: (200, 5.607179510205636)\n",
            "Losses for final trained model: (300, 5.502346399904426)\n",
            "Losses for final trained model: (400, 5.413353864503235)\n",
            "Losses for final trained model: (500, 5.338649361934525)\n",
            "Losses for final trained model: (0, 5.272849683885971)\n",
            "Losses for final trained model: (100, 5.212486553594957)\n",
            "Losses for final trained model: (200, 5.157329588348822)\n",
            "Losses for final trained model: (300, 5.108243533570284)\n",
            "Losses for final trained model: (400, 5.063919738734956)\n",
            "Losses for final trained model: (500, 5.023586087722049)\n",
            "Losses for final trained model: (0, 4.986770065832874)\n",
            "Losses for final trained model: (100, 4.952558728065385)\n",
            "Losses for final trained model: (200, 4.919953733465571)\n",
            "Losses for final trained model: (300, 4.8880886971583575)\n",
            "Losses for final trained model: (400, 4.856465624648853)\n",
            "Losses for final trained model: (500, 4.825222577313657)\n",
            "Losses for final trained model: (0, 4.795048899758088)\n",
            "Losses for final trained model: (100, 4.7664868373033515)\n",
            "Losses for final trained model: (200, 4.7399239737939896)\n",
            "Losses for final trained model: (300, 4.716037569979077)\n",
            "Losses for final trained model: (400, 4.694276489295695)\n",
            "Losses for final trained model: (500, 4.672641436599405)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEgx56Ccs4_g",
        "colab_type": "code",
        "outputId": "ac9aa4d7-99f5-4755-f5b7-e9c610799d3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# Generate loss vs epoch plot from final trained model\n",
        "generate_loss_epoch_plt(losses)"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEWCAYAAABollyxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLyKS4/4/LhsSnQuxz5oAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcdOTV6iip1U",
        "colab_type": "text"
      },
      "source": [
        "### Part a: We double then halve the original number of hidden units.\n",
        "\n",
        "### We then display a loss vs epoch plot and the text sampling results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IVFRNCOjbJy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "8826b8e6-f7a7-4701-c250-8f5b911d96b0"
      },
      "source": [
        "np.random.seed(10)\n",
        "# Train on a small subset of the data to see what happens for hidden dimension of 100 and \n",
        "# 30 epochs\n",
        "model = RNNVanilla(vocabulary_size, hidden_dim=200)\n",
        "losses = train_with_sgd(model, XTrain[:100], YTrain[:100], nepoch=30, evaluate_loss_after=1)"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-30 02:47:39: Loss after num_examples_seen=0 epoch=0: 8.987841\n",
            "2019-10-30 02:48:06: Loss after num_examples_seen=100 epoch=1: 8.974026\n",
            "2019-10-30 02:48:33: Loss after num_examples_seen=200 epoch=2: 8.951954\n",
            "2019-10-30 02:48:58: Loss after num_examples_seen=300 epoch=3: 6.856553\n",
            "2019-10-30 02:49:22: Loss after num_examples_seen=400 epoch=4: 6.107072\n",
            "2019-10-30 02:49:47: Loss after num_examples_seen=500 epoch=5: 5.856458\n",
            "2019-10-30 02:50:09: Loss after num_examples_seen=600 epoch=6: 5.704365\n",
            "2019-10-30 02:50:29: Loss after num_examples_seen=700 epoch=7: 5.600037\n",
            "2019-10-30 02:50:48: Loss after num_examples_seen=800 epoch=8: 5.508858\n",
            "2019-10-30 02:51:09: Loss after num_examples_seen=900 epoch=9: 5.407975\n",
            "2019-10-30 02:51:31: Loss after num_examples_seen=1000 epoch=10: 5.316871\n",
            "2019-10-30 02:51:52: Loss after num_examples_seen=1100 epoch=11: 5.255837\n",
            "2019-10-30 02:52:12: Loss after num_examples_seen=1200 epoch=12: 5.201469\n",
            "2019-10-30 02:52:32: Loss after num_examples_seen=1300 epoch=13: 5.155292\n",
            "2019-10-30 02:52:54: Loss after num_examples_seen=1400 epoch=14: 5.105854\n",
            "2019-10-30 02:53:15: Loss after num_examples_seen=1500 epoch=15: 5.067899\n",
            "2019-10-30 02:53:36: Loss after num_examples_seen=1600 epoch=16: 5.034787\n",
            "2019-10-30 02:53:56: Loss after num_examples_seen=1700 epoch=17: 5.015542\n",
            "2019-10-30 02:54:17: Loss after num_examples_seen=1800 epoch=18: 4.972586\n",
            "2019-10-30 02:54:39: Loss after num_examples_seen=1900 epoch=19: 4.935414\n",
            "2019-10-30 02:55:00: Loss after num_examples_seen=2000 epoch=20: 4.920888\n",
            "2019-10-30 02:55:20: Loss after num_examples_seen=2100 epoch=21: 4.860164\n",
            "2019-10-30 02:55:39: Loss after num_examples_seen=2200 epoch=22: 4.822256\n",
            "2019-10-30 02:55:59: Loss after num_examples_seen=2300 epoch=23: 4.804404\n",
            "2019-10-30 02:56:18: Loss after num_examples_seen=2400 epoch=24: 4.772416\n",
            "2019-10-30 02:56:38: Loss after num_examples_seen=2500 epoch=25: 4.757278\n",
            "2019-10-30 02:56:58: Loss after num_examples_seen=2600 epoch=26: 4.739341\n",
            "2019-10-30 02:57:20: Loss after num_examples_seen=2700 epoch=27: 4.693923\n",
            "2019-10-30 02:57:43: Loss after num_examples_seen=2800 epoch=28: 4.661043\n",
            "2019-10-30 02:58:04: Loss after num_examples_seen=2900 epoch=29: 4.630802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soX6Q0ZNyzQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "cbcbce76-f34e-4989-a920-9f71f4a1f72b",
        "id": "YOVquvyjyzk8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "model_num = 0\n",
        "generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence # 1 of length 8\n",
            "discussions , was only possessed attentive , loud\n",
            "Sentence # 2 of length 8\n",
            "empty into at they of . by .\n",
            "Sentence # 3 of length 13\n",
            "the of a height overboard no were sensible the part the of miles\n",
            "Sentence # 4 of length 15\n",
            "horn was drama evident while . . it of a were rise 35 was .\n",
            "Sentence # 5 of length 10\n",
            "not the was containing were were what the passage .\n",
            "Sentence # 6 of length 7\n",
            "charles northeast roaring did no fact great\n",
            "Sentence # 7 of length 10\n",
            "but was devastating after been if little to in .\n",
            "Sentence # 8 of length 7\n",
            "it after a was or a spherical\n",
            "Sentence # 9 of length 11\n",
            ", have ” is the great had came to great dangers\n",
            "Sentence # 10 of length 17\n",
            "farther it was a did may was to their expel balloon resounded october a , of covering\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "outputId": "cf684ad5-4643-4023-ab5c-db62c321c74b",
        "id": "nc7BH8Uhj4RD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# Print losses for final trained model\n",
        "for loss in losses:\n",
        "    print(\"Losses for final trained model: \" + str(loss))"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Losses for final trained model: (0, 8.987840778641202)\n",
            "Losses for final trained model: (100, 8.974026270746105)\n",
            "Losses for final trained model: (200, 8.951953773716216)\n",
            "Losses for final trained model: (300, 6.8565526659118285)\n",
            "Losses for final trained model: (400, 6.107071944137023)\n",
            "Losses for final trained model: (500, 5.8564580958492085)\n",
            "Losses for final trained model: (600, 5.704364653515928)\n",
            "Losses for final trained model: (700, 5.600037495280701)\n",
            "Losses for final trained model: (800, 5.508857840947021)\n",
            "Losses for final trained model: (900, 5.4079754040539205)\n",
            "Losses for final trained model: (1000, 5.316871200892547)\n",
            "Losses for final trained model: (1100, 5.255836657206071)\n",
            "Losses for final trained model: (1200, 5.201469173084967)\n",
            "Losses for final trained model: (1300, 5.155291922543004)\n",
            "Losses for final trained model: (1400, 5.105853978876643)\n",
            "Losses for final trained model: (1500, 5.067898625670266)\n",
            "Losses for final trained model: (1600, 5.034786613376984)\n",
            "Losses for final trained model: (1700, 5.015542496332777)\n",
            "Losses for final trained model: (1800, 4.972586350631354)\n",
            "Losses for final trained model: (1900, 4.935414068124531)\n",
            "Losses for final trained model: (2000, 4.920888283024637)\n",
            "Losses for final trained model: (2100, 4.860163629798662)\n",
            "Losses for final trained model: (2200, 4.822256383533942)\n",
            "Losses for final trained model: (2300, 4.804404064129814)\n",
            "Losses for final trained model: (2400, 4.7724163076015165)\n",
            "Losses for final trained model: (2500, 4.75727799650036)\n",
            "Losses for final trained model: (2600, 4.739341460697774)\n",
            "Losses for final trained model: (2700, 4.6939234931213605)\n",
            "Losses for final trained model: (2800, 4.661043254760139)\n",
            "Losses for final trained model: (2900, 4.6308016828187535)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "623baf34-7f1b-4f51-83e2-75a92d099af3",
        "id": "338GhWUlj6b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# Generate loss vs epoch plot from final trained model\n",
        "generate_loss_epoch_plt(losses)"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEWCAYAAABollyxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b9302566-bf6d-402b-8dd5-b341423ce137",
        "id": "prEPYLABmiI1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "np.random.seed(10)\n",
        "# Train on a small subset of the data to see what happens for hidden dimension of 100 and \n",
        "# 30 epochs\n",
        "model = RNNVanilla(vocabulary_size, hidden_dim=50)\n",
        "losses = train_with_sgd(model, XTrain[:100], YTrain[:100], nepoch=30, evaluate_loss_after=1)"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-30 02:59:23: Loss after num_examples_seen=0 epoch=0: 8.987286\n",
            "2019-10-30 02:59:30: Loss after num_examples_seen=100 epoch=1: 8.974989\n",
            "2019-10-30 02:59:37: Loss after num_examples_seen=200 epoch=2: 8.956639\n",
            "2019-10-30 02:59:44: Loss after num_examples_seen=300 epoch=3: 8.814319\n",
            "2019-10-30 02:59:51: Loss after num_examples_seen=400 epoch=4: 6.805232\n",
            "2019-10-30 02:59:58: Loss after num_examples_seen=500 epoch=5: 6.352553\n",
            "2019-10-30 03:00:05: Loss after num_examples_seen=600 epoch=6: 6.105913\n",
            "2019-10-30 03:00:12: Loss after num_examples_seen=700 epoch=7: 5.933703\n",
            "2019-10-30 03:00:19: Loss after num_examples_seen=800 epoch=8: 5.796248\n",
            "2019-10-30 03:00:26: Loss after num_examples_seen=900 epoch=9: 5.681134\n",
            "2019-10-30 03:00:33: Loss after num_examples_seen=1000 epoch=10: 5.584433\n",
            "2019-10-30 03:00:40: Loss after num_examples_seen=1100 epoch=11: 5.504887\n",
            "2019-10-30 03:00:47: Loss after num_examples_seen=1200 epoch=12: 5.438862\n",
            "2019-10-30 03:00:54: Loss after num_examples_seen=1300 epoch=13: 5.382862\n",
            "2019-10-30 03:01:01: Loss after num_examples_seen=1400 epoch=14: 5.334306\n",
            "2019-10-30 03:01:08: Loss after num_examples_seen=1500 epoch=15: 5.290910\n",
            "2019-10-30 03:01:15: Loss after num_examples_seen=1600 epoch=16: 5.250510\n",
            "2019-10-30 03:01:22: Loss after num_examples_seen=1700 epoch=17: 5.211418\n",
            "2019-10-30 03:01:29: Loss after num_examples_seen=1800 epoch=18: 5.172652\n",
            "2019-10-30 03:01:36: Loss after num_examples_seen=1900 epoch=19: 5.134059\n",
            "2019-10-30 03:01:43: Loss after num_examples_seen=2000 epoch=20: 5.096284\n",
            "2019-10-30 03:01:50: Loss after num_examples_seen=2100 epoch=21: 5.060297\n",
            "2019-10-30 03:01:57: Loss after num_examples_seen=2200 epoch=22: 5.026705\n",
            "2019-10-30 03:02:04: Loss after num_examples_seen=2300 epoch=23: 4.995538\n",
            "2019-10-30 03:02:11: Loss after num_examples_seen=2400 epoch=24: 4.966576\n",
            "2019-10-30 03:02:18: Loss after num_examples_seen=2500 epoch=25: 4.939590\n",
            "2019-10-30 03:02:25: Loss after num_examples_seen=2600 epoch=26: 4.914329\n",
            "2019-10-30 03:02:32: Loss after num_examples_seen=2700 epoch=27: 4.890485\n",
            "2019-10-30 03:02:39: Loss after num_examples_seen=2800 epoch=28: 4.867766\n",
            "2019-10-30 03:02:46: Loss after num_examples_seen=2900 epoch=29: 4.845947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W70Uis8zGRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d740ffe9-d296-41fd-bd0d-ce0941b262ee",
        "id": "_xLLaPZUzGm2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "model_num = 0\n",
        "generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)"
      ],
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence # 1 of length 24\n",
            "region of day that having height movement to to the was argument breeze , -- life be into movement examine sea day . .\n",
            "Sentence # 2 of length 10\n",
            "crushing which to without of lofty , than as balloon\n",
            "Sentence # 3 of length 13\n",
            "by murmuring had 24th have , in in sea was rate . .\n",
            "Sentence # 4 of length 8\n",
            "it of havana form no lofty as the\n",
            "Sentence # 5 of length 11\n",
            "bricks leveled everything air of feet a , sometimes sustained enterprise\n",
            "Sentence # 6 of length 8\n",
            "the to march thousand day the rejoined .\n",
            "Sentence # 7 of length 7\n",
            "kinds obliquely in discovered ’ of through\n",
            "Sentence # 8 of length 7\n",
            "the the year tempest none the the\n",
            "Sentence # 9 of length 12\n",
            "contrary the , to breeze sack . of lower above wind alone\n",
            "Sentence # 10 of length 12\n",
            "it is by itself 24th , mountains while for lofty ran the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "outputId": "a0867d2e-40cf-43ab-d417-cf330b7043df",
        "id": "vYS56rIsmlU_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# Print losses for final trained model\n",
        "for loss in losses:\n",
        "    print(\"Losses for final trained model: \" + str(loss))"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Losses for final trained model: (0, 8.987286366782675)\n",
            "Losses for final trained model: (100, 8.97498906636971)\n",
            "Losses for final trained model: (200, 8.956638780564761)\n",
            "Losses for final trained model: (300, 8.814319374882414)\n",
            "Losses for final trained model: (400, 6.805232137549876)\n",
            "Losses for final trained model: (500, 6.352553270499179)\n",
            "Losses for final trained model: (600, 6.105912796490565)\n",
            "Losses for final trained model: (700, 5.933702958949123)\n",
            "Losses for final trained model: (800, 5.796247651312062)\n",
            "Losses for final trained model: (900, 5.681133525169246)\n",
            "Losses for final trained model: (1000, 5.584433431593295)\n",
            "Losses for final trained model: (1100, 5.504887428005688)\n",
            "Losses for final trained model: (1200, 5.438862486098705)\n",
            "Losses for final trained model: (1300, 5.3828619365930095)\n",
            "Losses for final trained model: (1400, 5.334305643299944)\n",
            "Losses for final trained model: (1500, 5.290909708243728)\n",
            "Losses for final trained model: (1600, 5.250509978236971)\n",
            "Losses for final trained model: (1700, 5.211418069397645)\n",
            "Losses for final trained model: (1800, 5.17265173186299)\n",
            "Losses for final trained model: (1900, 5.134058786953821)\n",
            "Losses for final trained model: (2000, 5.096283819963461)\n",
            "Losses for final trained model: (2100, 5.060296687030271)\n",
            "Losses for final trained model: (2200, 5.026704665171926)\n",
            "Losses for final trained model: (2300, 4.9955384984842475)\n",
            "Losses for final trained model: (2400, 4.966576186985766)\n",
            "Losses for final trained model: (2500, 4.93959040717947)\n",
            "Losses for final trained model: (2600, 4.914328883322301)\n",
            "Losses for final trained model: (2700, 4.890485131029355)\n",
            "Losses for final trained model: (2800, 4.867766331607302)\n",
            "Losses for final trained model: (2900, 4.845947261081879)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "24ba6990-eec0-4f4f-9f3d-eda320001935",
        "id": "DFuO5kfvmm_2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# Generate loss vs epoch plot from final trained model\n",
        "generate_loss_epoch_plt(losses)"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEWCAYAAABollyxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njlPQi4jkuP8PBE385coOITkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTfXetQFnw-d",
        "colab_type": "text"
      },
      "source": [
        "### Part b: We double then halve the original length of sentences.\n",
        "\n",
        "### We then display a loss vs epoch plot and the text sampling results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "642681fc-0a50-475d-e09c-abe7e43a2ca1",
        "id": "3XzLR0Q_n-0W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "np.random.seed(10)\n",
        "# Train on a small subset of the data to see what happens for hidden dimension of 100 and \n",
        "# 30 epochs\n",
        "model = RNNVanilla(vocabulary_size, hidden_dim=100)\n",
        "losses = train_with_sgd(model, XTrain_double[:100], YTrain_double[:100], nepoch=30, evaluate_loss_after=1)"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-30 03:06:35: Loss after num_examples_seen=0 epoch=0: 8.987250\n",
            "2019-10-30 03:07:01: Loss after num_examples_seen=100 epoch=1: 8.971931\n",
            "2019-10-30 03:07:26: Loss after num_examples_seen=200 epoch=2: 6.870198\n",
            "2019-10-30 03:07:55: Loss after num_examples_seen=300 epoch=3: 6.207778\n",
            "2019-10-30 03:08:19: Loss after num_examples_seen=400 epoch=4: 6.011113\n",
            "2019-10-30 03:08:44: Loss after num_examples_seen=500 epoch=5: 5.901763\n",
            "2019-10-30 03:09:10: Loss after num_examples_seen=600 epoch=6: 5.811904\n",
            "2019-10-30 03:09:33: Loss after num_examples_seen=700 epoch=7: 5.744759\n",
            "2019-10-30 03:09:56: Loss after num_examples_seen=800 epoch=8: 5.697565\n",
            "2019-10-30 03:10:22: Loss after num_examples_seen=900 epoch=9: 5.669299\n",
            "2019-10-30 03:10:52: Loss after num_examples_seen=1000 epoch=10: 5.678890\n",
            "Setting learning rate to 0.002500\n",
            "2019-10-30 03:11:27: Loss after num_examples_seen=1100 epoch=11: 5.621277\n",
            "2019-10-30 03:11:59: Loss after num_examples_seen=1200 epoch=12: 5.613412\n",
            "2019-10-30 03:12:26: Loss after num_examples_seen=1300 epoch=13: 5.586978\n",
            "2019-10-30 03:12:52: Loss after num_examples_seen=1400 epoch=14: 5.561865\n",
            "2019-10-30 03:13:21: Loss after num_examples_seen=1500 epoch=15: 5.532551\n",
            "2019-10-30 03:13:46: Loss after num_examples_seen=1600 epoch=16: 5.530960\n",
            "2019-10-30 03:14:10: Loss after num_examples_seen=1700 epoch=17: 5.514691\n",
            "2019-10-30 03:14:33: Loss after num_examples_seen=1800 epoch=18: 5.497928\n",
            "2019-10-30 03:14:58: Loss after num_examples_seen=1900 epoch=19: 5.464397\n",
            "2019-10-30 03:15:21: Loss after num_examples_seen=2000 epoch=20: 5.443500\n",
            "2019-10-30 03:15:44: Loss after num_examples_seen=2100 epoch=21: 5.419449\n",
            "2019-10-30 03:16:07: Loss after num_examples_seen=2200 epoch=22: 5.405912\n",
            "2019-10-30 03:16:29: Loss after num_examples_seen=2300 epoch=23: 5.405516\n",
            "2019-10-30 03:16:52: Loss after num_examples_seen=2400 epoch=24: 5.366782\n",
            "2019-10-30 03:17:14: Loss after num_examples_seen=2500 epoch=25: 5.364082\n",
            "2019-10-30 03:17:38: Loss after num_examples_seen=2600 epoch=26: 5.368355\n",
            "Setting learning rate to 0.001250\n",
            "2019-10-30 03:18:00: Loss after num_examples_seen=2700 epoch=27: 5.353523\n",
            "2019-10-30 03:18:24: Loss after num_examples_seen=2800 epoch=28: 5.340524\n",
            "2019-10-30 03:18:47: Loss after num_examples_seen=2900 epoch=29: 5.296403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5vLwrB92AGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "99ce9513-7d6c-45dc-e266-8c534e0a69f8",
        "id": "98Lz_nUo2AZB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "model_num = 0\n",
        "generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence # 1 of length 18\n",
            "and have no regard by , sea be sustained , , such and in of that it zones\n",
            "Sentence # 2 of length 8\n",
            "nothing. immense the in to passed lighter .\n",
            "Sentence # 3 of length 10\n",
            "of their was is was , to energetic . .\n",
            "Sentence # 4 of length 7\n",
            "pierce at it to whole and in\n",
            "Sentence # 5 of length 8\n",
            "bolts sea it plaything , watercourse , .\n",
            "Sentence # 6 of length 14\n",
            "herds the havana to was eighteen terrible . , say horizontal if from part\n",
            "Sentence # 7 of length 7\n",
            "does extending last the atmosphere were .\n",
            "Sentence # 8 of length 8\n",
            "moderate was they balloon endeavored of resounded the\n",
            "Sentence # 9 of length 13\n",
            "only have removed lengthening to weight disasters from without less passengers . ,\n",
            "Sentence # 10 of length 11\n",
            "round passengers people which an of storm balloon hours the expedition\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "outputId": "59e96068-daf8-41d5-fb99-ffa27e30e3f7",
        "id": "9IzQrSENoBAX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# Print losses for final trained model\n",
        "for loss in losses:\n",
        "    print(\"Losses for final trained model: \" + str(loss))"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Losses for final trained model: (0, 8.987249769421336)\n",
            "Losses for final trained model: (100, 8.971931205774302)\n",
            "Losses for final trained model: (200, 6.870198459875511)\n",
            "Losses for final trained model: (300, 6.207778213944166)\n",
            "Losses for final trained model: (400, 6.011112673534861)\n",
            "Losses for final trained model: (500, 5.90176270803229)\n",
            "Losses for final trained model: (600, 5.8119042256115625)\n",
            "Losses for final trained model: (700, 5.744759387662695)\n",
            "Losses for final trained model: (800, 5.69756500338993)\n",
            "Losses for final trained model: (900, 5.669298814638054)\n",
            "Losses for final trained model: (1000, 5.678890411709938)\n",
            "Losses for final trained model: (1100, 5.6212768905364445)\n",
            "Losses for final trained model: (1200, 5.613411905233475)\n",
            "Losses for final trained model: (1300, 5.586977832825059)\n",
            "Losses for final trained model: (1400, 5.561864776336102)\n",
            "Losses for final trained model: (1500, 5.532550955168914)\n",
            "Losses for final trained model: (1600, 5.530960001564596)\n",
            "Losses for final trained model: (1700, 5.514690714705463)\n",
            "Losses for final trained model: (1800, 5.4979279858950205)\n",
            "Losses for final trained model: (1900, 5.464396526435478)\n",
            "Losses for final trained model: (2000, 5.443500253411943)\n",
            "Losses for final trained model: (2100, 5.41944900201416)\n",
            "Losses for final trained model: (2200, 5.405912131126813)\n",
            "Losses for final trained model: (2300, 5.405516229704605)\n",
            "Losses for final trained model: (2400, 5.366781704429244)\n",
            "Losses for final trained model: (2500, 5.364081613312387)\n",
            "Losses for final trained model: (2600, 5.368355024165001)\n",
            "Losses for final trained model: (2700, 5.3535234461288965)\n",
            "Losses for final trained model: (2800, 5.340524197835761)\n",
            "Losses for final trained model: (2900, 5.296402569373133)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "020334df-28a8-4935-a993-09faf162a467",
        "id": "DLaGYSlRoFXp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# Generate loss vs epoch plot from final trained model\n",
        "generate_loss_epoch_plt(losses)"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEWCAYAAACEz/viAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rgPrEBCroDCF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "2c8115f5-72b9-467b-fef1-c2c1749cfc10"
      },
      "source": [
        "np.random.seed(10)\n",
        "# Train on a small subset of the data to see what happens for hidden dimension of 100 and \n",
        "# 30 epochs\n",
        "model = RNNVanilla(vocabulary_size, hidden_dim=100)\n",
        "losses = train_with_sgd(model, XTrain_half[:100], YTrain_half[:100], nepoch=30, evaluate_loss_after=1)"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-30 03:19:11: Loss after num_examples_seen=0 epoch=0: 8.987116\n",
            "2019-10-30 03:19:18: Loss after num_examples_seen=100 epoch=1: 8.980293\n",
            "2019-10-30 03:19:25: Loss after num_examples_seen=200 epoch=2: 8.972622\n",
            "2019-10-30 03:19:33: Loss after num_examples_seen=300 epoch=3: 8.962758\n",
            "2019-10-30 03:19:40: Loss after num_examples_seen=400 epoch=4: 8.946729\n",
            "2019-10-30 03:19:47: Loss after num_examples_seen=500 epoch=5: 7.951856\n",
            "2019-10-30 03:19:54: Loss after num_examples_seen=600 epoch=6: 6.664597\n",
            "2019-10-30 03:20:00: Loss after num_examples_seen=700 epoch=7: 6.217599\n",
            "2019-10-30 03:20:07: Loss after num_examples_seen=800 epoch=8: 5.957195\n",
            "2019-10-30 03:20:14: Loss after num_examples_seen=900 epoch=9: 5.778880\n",
            "2019-10-30 03:20:21: Loss after num_examples_seen=1000 epoch=10: 5.646273\n",
            "2019-10-30 03:20:28: Loss after num_examples_seen=1100 epoch=11: 5.543695\n",
            "2019-10-30 03:20:34: Loss after num_examples_seen=1200 epoch=12: 5.461293\n",
            "2019-10-30 03:20:41: Loss after num_examples_seen=1300 epoch=13: 5.392851\n",
            "2019-10-30 03:20:48: Loss after num_examples_seen=1400 epoch=14: 5.334859\n",
            "2019-10-30 03:20:54: Loss after num_examples_seen=1500 epoch=15: 5.285190\n",
            "2019-10-30 03:21:00: Loss after num_examples_seen=1600 epoch=16: 5.242233\n",
            "2019-10-30 03:21:06: Loss after num_examples_seen=1700 epoch=17: 5.204631\n",
            "2019-10-30 03:21:12: Loss after num_examples_seen=1800 epoch=18: 5.171263\n",
            "2019-10-30 03:21:18: Loss after num_examples_seen=1900 epoch=19: 5.141245\n",
            "2019-10-30 03:21:24: Loss after num_examples_seen=2000 epoch=20: 5.113875\n",
            "2019-10-30 03:21:29: Loss after num_examples_seen=2100 epoch=21: 5.088471\n",
            "2019-10-30 03:21:36: Loss after num_examples_seen=2200 epoch=22: 5.064208\n",
            "2019-10-30 03:21:42: Loss after num_examples_seen=2300 epoch=23: 5.040373\n",
            "2019-10-30 03:21:49: Loss after num_examples_seen=2400 epoch=24: 5.016940\n",
            "2019-10-30 03:21:56: Loss after num_examples_seen=2500 epoch=25: 4.994532\n",
            "2019-10-30 03:22:02: Loss after num_examples_seen=2600 epoch=26: 4.973730\n",
            "2019-10-30 03:22:09: Loss after num_examples_seen=2700 epoch=27: 4.954018\n",
            "2019-10-30 03:22:16: Loss after num_examples_seen=2800 epoch=28: 4.933887\n",
            "2019-10-30 03:22:23: Loss after num_examples_seen=2900 epoch=29: 4.912587\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrTh6YvI2JLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7c036406-8451-4b6d-d1b9-b06007be92b2",
        "id": "2tJRmKPe2Jcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "model_num = 0\n",
        "generate_sentences_with_model(model, num_sentences, senten_min_length, model_num)"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence # 1 of length 8\n",
            "dropped ! could . movement descending their as\n",
            "Sentence # 2 of length 7\n",
            "in tempest , it have of that\n",
            "Sentence # 3 of length 7\n",
            ", , . of a horizontal to\n",
            "Sentence # 4 of length 7\n",
            "it feet to of only balloon could\n",
            "Sentence # 5 of length 12\n",
            "the ” of the their is noise from dashing was low zones\n",
            "Sentence # 6 of length 11\n",
            "accelerated were while traversed alone every rate a taken to a\n",
            "Sentence # 7 of length 14\n",
            "language we ” the , sensible vast while balloon year changed . in the\n",
            "Sentence # 8 of length 7\n",
            "planted diminished the solidity did and equator\n",
            "Sentence # 9 of length 15\n",
            "suspecting was discovered their hours may which less thousand taken hauling hours was day call\n",
            "Sentence # 10 of length 14\n",
            "were is the only miles lofty some could ; beneath the waves of the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "outputId": "18ad85cc-a00c-4909-834d-bb5c983be6d3",
        "id": "BsnUlbAMoBMk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# Print losses for final trained model\n",
        "for loss in losses:\n",
        "    print(\"Losses for final trained model: \" + str(loss))"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Losses for final trained model: (0, 8.987115686118328)\n",
            "Losses for final trained model: (100, 8.980293262135893)\n",
            "Losses for final trained model: (200, 8.972622141603473)\n",
            "Losses for final trained model: (300, 8.962758142989374)\n",
            "Losses for final trained model: (400, 8.946729329272799)\n",
            "Losses for final trained model: (500, 7.951855843574217)\n",
            "Losses for final trained model: (600, 6.664596967785795)\n",
            "Losses for final trained model: (700, 6.2175994731360325)\n",
            "Losses for final trained model: (800, 5.957195060951127)\n",
            "Losses for final trained model: (900, 5.778879960268397)\n",
            "Losses for final trained model: (1000, 5.646273492651651)\n",
            "Losses for final trained model: (1100, 5.543694542874483)\n",
            "Losses for final trained model: (1200, 5.461293285868861)\n",
            "Losses for final trained model: (1300, 5.392850711713368)\n",
            "Losses for final trained model: (1400, 5.334859252838354)\n",
            "Losses for final trained model: (1500, 5.28519027953983)\n",
            "Losses for final trained model: (1600, 5.242233242099906)\n",
            "Losses for final trained model: (1700, 5.2046311539743515)\n",
            "Losses for final trained model: (1800, 5.171263076705106)\n",
            "Losses for final trained model: (1900, 5.141244504629941)\n",
            "Losses for final trained model: (2000, 5.113874514084146)\n",
            "Losses for final trained model: (2100, 5.088470662973693)\n",
            "Losses for final trained model: (2200, 5.064208264092143)\n",
            "Losses for final trained model: (2300, 5.040373038490393)\n",
            "Losses for final trained model: (2400, 5.016939542472966)\n",
            "Losses for final trained model: (2500, 4.994532084353671)\n",
            "Losses for final trained model: (2600, 4.973729910970518)\n",
            "Losses for final trained model: (2700, 4.954018075329183)\n",
            "Losses for final trained model: (2800, 4.933887228802783)\n",
            "Losses for final trained model: (2900, 4.912586670996985)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e8055a90-a568-4729-ec7e-9727a3cb21ec",
        "id": "HJ59i031oFij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# Generate loss vs epoch plot from final trained model\n",
        "generate_loss_epoch_plt(losses)"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEWCAYAAACEz/viAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZhcdZ3v8fe3qnrvSncn6SVLd0KS\nDsiaQIsSUFlUBkZhFES4MoobRsFldJyr89yrDnfUWRxnRBQMbriAC4uiooIgKIiBTkggYctCls7S\n3Unv+/a9f9RJUjTdne6kT6qr+vN6nnrq1DmnTn8PRepT5/c75/zM3RERkektkuoCREQk9RQGIiKi\nMBAREYWBiIigMBARERQGIiKCwkAk45nZNWb2aKrrkKlNYSBTnpltM7M3prqOyWBm55rZkJl1DHuc\nleraZHqLpboAkWlot7vPT3URIsl0ZCBpzcw+aGabzazJzO41s7nBfDOz/zazBjNrM7NnzOzkYNnF\nZvasmbWb2S4z+8cRtptjZi0H3hPMKzWzbjMrM7PZZvbrYJ0mM/uzmR31vycze9jMvmxmTwR1/9LM\nZiYtv8TMNgZ/92Eze1XSskozu9vMGs1sv5ndNGzbXzGzZjN7ycwuOtpaJbMoDCRtmdn5wJeBK4A5\nwHbgJ8HiNwOvB5YCRcE6+4Nl3wE+5O5x4GTgoeHbdvde4G7gqqTZVwCPuHsD8CmgDigFyoF/Bibr\n3i7vBt4X7NMAcCOAmS0F7gA+Efzd+4BfmVm2mUWBX5P4b7AQmMeh/xYArwFeAGYD/wF8x8xskuqV\nDKAwkHT2LuC77r42+PL+LHCWmS0E+oE4cAJg7v6cu+8J3tcPnGhmM9y92d3XjrL924Erk17/r2De\ngW3MARa4e7+7/9nHf6OvucEv++RHQdLyH7r7BnfvBP4vcEXwZf9O4Dfu/oC79wNfAfKAFcCZwFzg\n0+7e6e497p7cabzd3W9190HgtqD28nHWK9OAwkDS2VwSv4QBcPcOEr/+57n7Q8BNwDeABjNbZWYz\nglUvAy4GtpvZI2N03v4RyDez1wQBswy4J1j2n8Bm4H4z22pmn5lA3bvdvXjYozNp+c6k6e1AFolf\n9MP3dyhYdx5QSeILf2CUv7k36X1dwWThBGqWDKcwkHS2G1hw4EXw63oWsAvA3W909zOAE0k0F306\nmP+ku18KlAG/AH420saDX9E/I9FUdBXwa3dvD5a1u/un3H0RcAnwSTO7YJL2qzJpuorEUci+EfbX\ngnV3kQiFKjPTSSFyRBQGki6yzCw36REj0X7+XjNbZmY5wJeA1e6+zcxeHfyizwI6gR5gKGhff5eZ\nFQVNLW3A0Bh/93YSzTPv4lATEWb2FjNbEnwhtwKDh9nORFxtZieaWT5wA3BnUjD9rZldEOzXp4Be\n4C/AE8Ae4N/MrCD4b3T2JNUj04DCQNLFfUB30uML7v4HEm3qd5H4IlzMoTb+GcCtQDOJppX9JJp2\nAP4e2GZmbcBKEl/0I3L31STCZC7w26RF1cAfgA7gceCb7v5HADP7rZn98xj7MneE6wwuS1r+Q+D7\nJJp2coGPBbW8AFwNfJ3EkcJbgbe6e18QFm8FlgA7SHRuv3OMGkRexjS4jcjUYWYPAz9y92+nuhaZ\nXnRkICIiCgMREVEzkYiIoCMDEREhDW9UN3v2bF+4cGGqyxARSStr1qzZ5+6loy1PuzBYuHAhtbW1\nqS5DRCStmNn2sZarmUhERBQGIiKiMBARERQGIiJCyGFgZh83sw3ByEyfGGG5mdmNwUhVT5vZ6WHW\nIyIiIwstDILhAj9IYtCN04C3mNmSYatdROKGX9XAtcDNYdUjIiKjC/PI4FUkbifcFQy48Qjw9mHr\nXAr8wBP+ChSb2ZwQaxIRkRGEeZ3BBuCLZjaLxC2HLwaGXyAwj5eP6lQXzNvDJHuxvp1fr99NTlaU\nnFgkeETJyUp+DqZjEXKD+XnZUXKzouTGIsSi6mIRkcwUWhi4+3Nm9u/A/STuB7+OxAAgE2Zm15Jo\nRqKqquqI6nmxvp0bH9p8RO89IBaxRDC8LCgi5GfHyM+OUpAdIy87SkF2lPycGPlZwXN2lPzsKDNy\nsyjOz6IkP5uS/GziuTEiEY1JLiKpd8xuVGdmXwLq3P2bSfO+BTzs7ncEr18Azk0auPwVampq/Eiv\nQHZ3+gaH6B0Yord/iN6BwVdODwzR0z9IT/8gvf1D9AwMBq+HDj0PHFre3T9IV98AXX2DiUfvAJ19\ng3T3DdI3OPbAV9GIUZSXHBBZFOdnUzUzn+vOW0JUQSEik8TM1rh7zWjLQ70dhZmVuXuDmVWR6C94\n7bBV7gWuN7OfAK8BWscKgkmoJ2gGiibGjwpZ38AQ3X2DdPYN0NU3QGv3AC1dfTR39QfPSdOd/dQ1\nd7NuZyv7Ono5a/EsXr1wZvhFiogQ/r2J7gr6DPqB69y9xcxWArj7LSSGMrwY2Ax0Ae8NuZ5jKjsW\nITsWoSg/a9zvaWzv5dVf/APrdrQoDETkmAk1DNz9dSPMuyVp2oHrwqwh3ZTGc5hfkse6nS2pLkVE\nphGdHjMFLass5qkdzakuQ0SmEYXBFLS8qoTdrT3Ut/WkuhQRmSYUBlPQ8qpiAJ7aoaYiETk2FAZT\n0IlzZpAVNZ7aqaYiETk2FAZTUG5WlBPnFunIQESOGYXBFLW8sphn6loZOMyFayIik0FhMEUtryqm\nu3+QF+rbU12KiEwDCoMpanllCaBOZBE5NhQGU1TlzDxmFWTr4jMROSYUBlOUmeniMxE5ZhQGU9jy\nqmK2NHbS2tWf6lJEJMMpDKaw5VWJfoP1dWoqEpFwKQymsFPnF2GmTmQRCZ/CYAqL52ZRXVaoK5FF\nJHQKgylueWUJ63a2cKxGpBOR6UlhMMUtqyqmpaufbfu7Ul2KiGSwUMPAzP7BzDaa2QYzu8PMcoct\nv8bMGs1sXfD4QJj1pKNDdzBVU5GIhCe0MDCzecDHgBp3PxmIAleOsOpP3X1Z8Ph2WPWkq+qyOAXZ\nUV18JiKhCruZKAbkmVkMyAd2h/z3Mk40Ypw6v1hnFIlIqEILA3ffBXwF2AHsAVrd/f4RVr3MzJ42\nszvNrHKkbZnZtWZWa2a1jY2NYZU8ZS2vKua5PW109w2muhQRyVBhNhOVAJcCxwFzgQIzu3rYar8C\nFrr7qcADwG0jbcvdV7l7jbvXlJaWhlXylLW8qoSBIWfD7tZUlyIiGSrMZqI3Ai+5e6O79wN3AyuS\nV3D3/e7eG7z8NnBGiPWkrWWViU7kdWoqEpGQhBkGO4DXmlm+mRlwAfBc8gpmNifp5SXDl0tCaTyH\n+SV5uvhMREITC2vD7r7azO4E1gIDwFPAKjO7Aah193uBj5nZJcHyJuCasOpJd8urSqjd1pTqMkQk\nQ4UWBgDu/nng88Nmfy5p+WeBz4ZZQ6ZYVlnMr9bvZm9rDxVFuYd/g4jIBOgK5DRx4OKzdWoqEpEQ\nKAzSxElzZ5Adjeh6AxEJhcIgTeTEorxq7gye0pXIIhIChUEaWV5ZzNN1LQwMDqW6FBHJMAqDNLK8\nqpie/iGe39ue6lJEJMMoDNLI8srEMJi6aZ2ITDaFQRqpnJnHrIJsdSKLyKRTGKQRM2N5VbGuRBaR\nSacwSDPLKovZ2thJa1d/qksRkQyiMEgzy6uCfoM6NRWJyORRGKSZU+cXYaZhMEVkcikM0kw8N4vq\nskKdUSQik0phkIaWV5bw1I4W3D3VpYhIhlAYpKHlVcW0dvfz0r7OVJciIhlCYZCGlh28g6maikRk\ncoQaBmb2D2a20cw2mNkdZpY7bHmOmf3UzDab2WozWxhmPZmiuixOQXZUF5+JyKQJLQzMbB7wMaDG\n3U8GosCVw1Z7P9Ds7kuA/wb+Pax6Mkk0YpxWqYvPRGTyhN1MFAPyzCwG5AO7hy2/FLgtmL4TuCAY\nL1kOY1llMc/vaae7bzDVpYhIBggtDNx9F/AVYAewB2h19/uHrTYP2BmsPwC0ArOGb8vMrjWzWjOr\nbWxsDKvktLK8qoSBIWfD7tZUlyIiGSDMZqISEr/8jwPmAgVmdvWRbMvdV7l7jbvXlJaWTmaZaWtZ\nZaITWRefichkCLOZ6I3AS+7e6O79wN3AimHr7AIqAYKmpCJgf4g1ZYzSeA7zivNYX6cjAxE5emGG\nwQ7gtWaWH/QDXAA8N2yde4H3BNOXAw+5rqQatxMq4myq10A3InL0wuwzWE2iU3gt8Ezwt1aZ2Q1m\ndkmw2neAWWa2Gfgk8Jmw6slE1eVxXtrXSb+GwRSRoxQLc+Pu/nng88Nmfy5peQ/wjjBryGRLywvp\nH3S27eukujye6nJEJI3pCuQ0tjQIgBfrO1JciYikO4VBGltcWogZvKB+AxE5SgqDNJaXHaVqZr46\nkUXkqCkM0lx1WZwXFQYicpQUBmluaXkh2/Z30Tug21KIyJFTGKS54yviDA65xjYQkaOiMEhz1WU6\no0hEjp7CIM0tKi0gYqgTWUSOisIgzeVmRVk4q0CdyCJyVBQGGaC6vJBNaiYSkaOgMMgAS8vjbNvf\nSU+/zigSkSOjMMgA1eVxhhy2NuqMIhE5MgqDDLC0vBCATQ3qNxCRI6MwyADHzS4gGjF1IovIEVMY\nZICcWJSFs/J1rYGIHLEwx0A+3szWJT3azOwTw9Y518xak9b53Gjbk7EtLdeoZyJy5EIb3MbdXwCW\nAZhZlMR4x/eMsOqf3f0tYdUxXVSXx/ndxr309A+SmxVNdTkikmaOVTPRBcAWd99+jP7etLO0vBB3\n2NygpiIRmbhjFQZXAneMsuwsM1tvZr81s5NGWsHMrjWzWjOrbWxsDK/KNHZg1DOdUSQiRyL0MDCz\nbOAS4OcjLF4LLHD304CvA78YaRvuvsrda9y9prS0NLxi09jCWQXEIqZOZBE5IsfiyOAiYK271w9f\n4O5t7t4RTN8HZJnZ7GNQU8bJjkU4bnaBOpFF5IgcizC4ilGaiMyswswsmD4zqGf/MagpIy0tj+vI\nQESOSKhhYGYFwJuAu5PmrTSzlcHLy4ENZrYeuBG40t09zJoyWXV5ITubu+ju0z2KRGRiQju1FMDd\nO4FZw+bdkjR9E3BTmDVMJ0vL4wfPKDplflGqyxGRNKIrkDPIgXsU6bYUIjJRCoMMsmBWAVlR40Wd\nXioiE6QwyCBZ0QiLZmugGxGZOIVBhqkuL1QzkYhMmMIgwywtj1PX3E1n70CqSxGRNKIwyDAHOpF1\njyIRmQiFQYapDu5RpKYiEZkIhUGGWTAzn+xohE06MhCRCVAYZJhYNMKi0gIdGYjIhCgMMlBi1DMd\nGYjI+CkMMtDS8kJ2tXTToTOKRGScFAYZ6EAnsm5nLSLjpTDIQAdHPVNTkYiM07jCwMwWm1lOMH2u\nmX3MzIrDLU2OVNXMfHJiEXUii8i4jffI4C5g0MyWAKuASuD20KqSoxKNGItLC3lRp5eKyDiNNwyG\n3H0AeBvwdXf/NDBnrDeY2fFmti7p0WZmnxi2jpnZjWa22cyeNrPTj2w3ZLil5YXqMxCRcRtvGPSb\n2VXAe4BfB/OyxnqDu7/g7svcfRlwBtAF3DNstYuA6uBxLXDzeAuXsVWXx9nT2kNbT3+qSxGRNDDe\nMHgvcBbwRXd/ycyOA344gb9zAbDF3bcPm38p8ANP+CtQbGZjHnHI+KgTWUQmYlxh4O7PuvvH3P0O\nMysB4u7+7xP4O1cCd4wwfx6wM+l1XTBPjpJGPRORiRjv2UQPm9kMM5sJrAVuNbOvjvO92cAlwM+P\ntEgzu9bMas2strGx8Ug3M61UluSTm6UzikRkfMbbTFTk7m3A20k067wGeOM433sRsNbd60dYtovE\nmUkHzA/mvYy7r3L3GnevKS0tHeefnd4iEWNJmUY9E5HxGW8YxIK2/Cs41IE8XlcxchMRwL3Au4Oz\nil4LtLr7ngluX0axtCyuIwMRGZfxhsENwO9JdAI/aWaLgE2He5OZFQBvAu5OmrfSzFYGL+8DtgKb\ngVuBj0ygdjmM6vI4De29tHbpjCIRGVtsPCu5+89JavN3963AZeN4Xycwa9i8W5KmHbhuvMXKxBzs\nRG5o59ULZ6a4GhGZysbbgTzfzO4xs4bgcZeZzQ+7ODk6SzXqmYiM03ibib5Hon1/bvD4VTBPprB5\nxXnkZUXViSwihzXeMCh19++5+0Dw+D6g03qmuEjEqC4v1JGBiBzWeMNgv5ldbWbR4HE1sD/MwmRy\nVJfFeVFHBiJyGOMNg/eROK10L7AHuBy4JqSaZBIdX1HIvo5emjv7Ul2KiExh470dxXZ3v8TdS929\nzN3/jnGcTSSpV61OZBEZh6MZ6eyTk1aFhObgGUUa20BExnA0YWCTVoWEZm5RLoU5MY1tICJjOpow\n8EmrQkJjlrhHkZqJRGQsY16BbGbtjPylb0BeKBXJpFtaXsiDzzWkugwRmcLGPDJw97i7zxjhEXf3\ncd3KQlJvaXmc/Z19NLT1pLoUEZmijqaZSNLEG5Ymrg/80eodKa5ERKYqhcE0UF0e58KTyvn+Yy9p\nTGQRGZHCYJq4/rxq2noG+OHjw4ehFhFRGEwbp8wv4tzjS/nOoy/R1TeQ6nJEZIpRGEwjHz1/CU2d\nfdyuvgMRGSbUMDCzYjO708yeN7PnzOysYcvPNbNWM1sXPD4XZj3T3RkLZnLWolms+tNWevoHU12O\niEwhYR8ZfA34nbufAJwGPDfCOn9292XB44aQ65n2rj9/CQ3tvdy5pi7VpYjIFBJaGJhZEfB64DsA\n7t7n7i1h/T0ZnxWLZ7G8qpibH95C/+BQqssRkSkizCOD44BG4Htm9pSZfdvMCkZY7ywzW29mvzWz\nk0bakJlda2a1Zlbb2NgYYsmZz8z46PlL2NXSzS+e2pXqckRkiggzDGLA6cDN7r4c6AQ+M2ydtcAC\ndz8N+Drwi5E25O6r3L3G3WtKSzXA2tE67/gyTpwzg28+vIXBId1iSkTCDYM6oM7dVwev7yQRDge5\ne5u7dwTT9wFZZjY7xJqEQ0cHL+3r5DfP7El1OSIyBYQWBu6+F9hpZscHsy4Ank1ex8wqzMyC6TOD\nejSc5jFw4UkVLCkr5BsPbWZIRwci017YZxN9FPixmT0NLAO+ZGYrzWxlsPxyYIOZrQduBK50d30z\nHQORiHH9eUt4ob6dB56rT3U5IpJilm7fvTU1NV5bW5vqMjLCwOAQF3z1EYrysvjldWcTHKSJSAYy\nszXuXjPacl2BPI3FohE+cu5inq5r5U+b9qW6HBFJIYXBNPe25fOZW5TL1x/cRLodJYrI5FEYTHPZ\nsQgrz11M7fZmVr/UlOpyRCRFFAbCFTWVlMZzuOmhzakuRURSRGEg5GZF+eDrjuPRzftYu6M51eWI\nSAooDASAd71mAcX5WXxDRwci05LCQAAoyInx/rOP48HnG9i4uzXV5YjIMaYwkIPevWIh8ZwY/3Lv\ns/QN6I6mItOJwkAOKsrL4l/fdjJPbGvic7/coFNNRaaRWKoLkKnl0mXz2FTfwU1/3MzS8jjvO+e4\nVJckIseAjgzkFT75pqVceFI5//qbZ3n4hYZUlyMix4DCQF4hEjG+esUyjq+YwUdvf4rNDe2pLklE\nQqYwkBEV5MT49ntqyMmK8P7bamnu7Et1SSISIoWBjGpecR7f+vsa9rT0cN3tazVmskgGUxjImM5Y\nUMKX334Kf9myny/cu1FnGIlkqFDDwMyKzexOM3vezJ4zs7OGLTczu9HMNpvZ02Z2+mjbktS57Iz5\nfOgNi/jx6h384PHtqS5HREIQ9qmlXwN+5+6Xm1k2kD9s+UVAdfB4DXBz8CxTzD9deAJbGjq44dfP\nsqi0gNdVl6a6JBGZRKEdGZhZEfB64DsA7t7n7i3DVrsU+IEn/BUoNrM5YdUkRy4aMf7nyuUsKS3k\nuh+vZWtjR6pLEpFJFGYz0XFAI/A9M3vKzL5tZgXD1pkH7Ex6XRfMexkzu9bMas2strGxMbyKZUyF\nwRlGsWiED9xWS2tXf6pLEpFJEmYYxIDTgZvdfTnQCXzmSDbk7qvcvcbda0pL1TyRSpUz8/nW35/B\nzuYuPnL7Gnr6B1NdkohMgjDDoA6oc/fVwes7SYRDsl1AZdLr+cE8mcJevXAmX3rbKTy2eT/vXPVX\nGtp6Ul2SiByl0MLA3fcCO83s+GDWBcCzw1a7F3h3cFbRa4FWd98TVk0yed5RU8ktV5/Bpvp23nrT\nozxdN7w7SETSSdjXGXwU+LGZPQ0sA75kZivNbGWw/D5gK7AZuBX4SMj1yCT6m5MruHPlCmKRCO+4\n5XHuXb871SWJyBGydLuIqKamxmtra1NdhiTZ19HLh3+0hie3NXPdeYv51JuOJxKxVJclIknMbI27\n14y2XFcgy1GbXZjDjz/wWt5ZU8k3/riFD/1oDR29A6kuS0QmQGEgkyI7FuHfLjuFz7/1RB58rp7L\nb/4LO5u6Ul2WiIyTwkAmjZnx3rOP4/vvPZPdLd1c+o3HWL11f6rLEpFxUBjIpHv90lJ+cd3ZFOdl\n8a5vr+aOJ3akuiQROQyFgYRiUWkh91x3NiuWzOazdz/DP/58vcZEEJnCFAYSmqK8LL77nhquO28x\nv3hqF+f/18P87MmdDA2l1xlsItOBwkBCFYtG+PSFJ/Cbj72O6rI4/3TX01zxrcd5fm9bqksTkSQK\nAzkmjq+I89MPvZb/vPxUtu7r5G9vfJQv/uZZOnUKqsiUoDCQY8bMeEdNJQ9+8g1cUTOfW//8Em/8\n6iP8bsMejaAmkmIKAznmSgqy+fLbT+WuD6+gOD+blT9ay/u+/yQ79uu6BJFUURhIypyxoIRfXX82\n/+dvX8UTLzXxpv9+hBsf3KSrl0VSQPcmkilhT2s3/+/Xz3LfM3spysvimhULuWbFQkoKslNdmkhG\nONy9iRQGMqU8taOZbz68hQeerSc/O8pVZ1bxwdctoqIoN9WliaQ1hYGkpRf2tnPLI1u4d/1uIgaX\nnT6flW9YzMLZw0dOFZHxUBhIWtvZ1MW3/rSFn9XWMTA4xMWnzOEj5y7hxLkzUl2aSFpJaRiY2Tag\nHRgEBoYXYmbnAr8EXgpm3e3uN4y1TYXB9NTQ3sN3H93Gj/66nY7eAc47vpT3n7OIFYtnaewEkXGY\nCmFQ4+77Rll+LvCP7v6W8W5TYTC9tXb388PHt/Hdx7bR1NnHvOI8LjtjPu84Yz6VM/NTXZ7IlHW4\nMIgdy2JEjlZRXhbXn1/NB163iPufrefntTv5+kObuPHBTZy1aBZXvHo+f3PSHPKyo6kuVSSthH1k\n8BLQDDjwLXdfNWz5ucBdQB2wm8RRwsYRtnMtcC1AVVXVGdu3bw+tZkk/u1q6uXtNHT9fU8eOpi7i\nOTHectpc3lEzn+WVxZipGUkk1c1E89x9l5mVAQ8AH3X3PyUtnwEMuXuHmV0MfM3dq8fappqJZDRD\nQ84T25r4We1OfvvMXrr7B1lSVshlp8/n4lMqWDBLZyLJ9DVlziYysy8AHe7+lTHW2cYYfQygMJDx\nae/p5zdP7+Hna+pYs70ZgBMq4lx4UgUXnlTBq+bEdcQg00rKwsDMCoCIu7cH0w8AN7j775LWqQDq\n3d3N7EzgTmCBj1GUwkAmamdTF7/fuJf7N9bz5PYm3KFyZh4XnljBhSdXcHpVCVGdkSQZLpVhsAi4\nJ3gZA2539y+a2UoAd7/FzK4HPgwMAN3AJ939L2NtV2EgR2NfRy9/eLae32/cy2Ob99M3OMTswmze\ndGI5bz6pghWLZ5ETU+ezZJ4p00w0WRQGMlnae/p5+IVGfr9xLw+/0EhH7wC5WRHOPG4W5yyZxTlL\nSjmhIq7rGCQjKAxExqF3YJC/bN7PnzY18uimfWxq6ABgdmE2KxbP5pzq2ZyzZDZzi/NSXKnIkdF1\nBiLjkBOLct4JZZx3QhkAe1t7eGzzPh4NHveu3w3AotICzlkym7OXzKZmQQmzCnNSWbbIpNGRgchh\nuDsv1nfw502NPLZ5H6tfaqKrbxCARbMLOH1BCTULSqhZWMKi2YVqVpIpSc1EIpOsb2CIp+taqN3e\nTO22ZtZsb6K5qx+A4vwsTq8q4YwgIE6rLCY3Sx3SknpqJhKZZNmxCDULZ1KzcCa8IXHksHVfJ2u2\nNbNmezO125t46PkGAGIR44Q5cU6ZV8TJ84o4ZV4Rx1fEdcaSTDk6MhAJQVNnH2u3N1O7vZlndrXw\nTF0rbT2J4TyzosbS8kMBcfK8Ik6oiOsIQkKlZiKRKcDd2dnUzYbdrTyzq5UNuxLPLUHzUixiLCkr\n5FVzZrC0PM4JFXGWVsSZW5SrK6VlUqiZSGQKMDOqZuVTNSufi0+ZAyQCYldL98Fg2Li7jdVb93PP\nU7sOvi+eE2NpRfxQQATPGhtaJpuODESmmNbufjbVt/P83nZeDJ5f2NtOa3f/wXVmF2azaHYhi8sK\nXvY8vySPWDSSwuplqtKRgUiaKcrLOtRBHXB3Gtp7g2BoY0tDJ1v3dfD7jfU0de48uF52NMKCWfks\nLi1kUWkBi0oLWRgckZQW5qjJSUalMBBJA2ZG+Yxcymfk8oalpS9b1tzZx9Z9HWxp7GRLYwdbGzt5\nsaGdPzxXz8DQoSP//OwoVTPzWTirgAWz8llw8DmfOUV5ulnfNKcwEElzJQXZnFEwkzMWzHzZ/P7B\nIXY2dbG9qYvt+zoTz/u72NTQzkPPN9A3OHRw3exohHklecwvyWN+ST7zS/KonJkfvM7TUcU0oDAQ\nyVBZ0QiLSgtZVFoIx7982eCQs7et52BIbNvfSV1TN3XNXdy/ey/7O/tetn5OLPKygJhbnMe84sTz\n3OI8yuM56qtIcwoDkWkoGjHmBV/oK0ZY3tk7wK6WbnY2dVHXnAiJnU3d1LV0sW5ny8FTYg+IGFTM\nyD0YDomwyGVOUR5zinOZW5RHcX6Wji6mMIWBiLxCQU6MpeWJU1lH0tk7wJ7Wbna19LC7pZvdLd3s\nCp7X7Wzhtxv20D/48jMVc7MiiXAoSoTE3OJcKooSQTGnOJc5M/KYkRdTYKSIwkBEJqwgJ8aSsjhL\nykYOi6Ehp7Gjl90t3ext7WF3aw97WrrZ09rDntZu/rJlH/VtPQwNO7M9PztKRVEuc4pyqZhxKDAO\nvJ5TlKsjjJCEGgbBmMbtwC5PYv8AAAhVSURBVCAwMPwcV0t8ol8DLga6gGvcfW2YNYlI+CKRQ2c/\njWZgcCgIjERA7G3tYU9rT/DczeNb9lHf3svgsMTIiUUS4RAcYZTPyD34uiKYnlWYo7OjJuhYHBmc\nN8YA9xcB1cHjNcDNwbOIZLhY9ECzUR5QMuI6g0NOY3vvwbDY3dpDfVsiNOpbe6jd3kR9a+/LzoyC\nxO09yuI5lBflUh5PBEXZjBwqgoBKPHKI52Ydgz1ND6luJroU+IEnLoP+q5kVm9kcd9+T4rpEZAqI\nRizxi79o9COMoSGnqauPvQeOKtp62Nvazd7WXurbetjS2MFjW/bRHtwoMFlBdpTyGYmgKI3nUhbP\noTSeQ1k8h7J47sHp6dA0FXYYOHC/mTnwLXdfNWz5PGBn0uu6YN7LwsDMrgWuBaiqqgqvWhFJO5GI\nMbswh9mFOZw8r2jU9br6Bqhv62Vvaw8N7YngqG9LBEZDew/P1LXQ0N57cOCiZFlRo7QwERSl8ZyD\nf+/QdHZiOp5DPCc9O8HDDoNz3H2XmZUBD5jZ8+7+p4luJAiRVZC4N9FkFykimS8/O8Zxs2McN7tg\nzPU6egdobO+loa2Hxo5eGtp6Dz43tPewq6WH9XWt7O/ofUUHOCTGuyhNCooDAVIazzkUKMFzXvbU\nuW15qGHg7ruC5wYzuwc4E0gOg11AZdLr+cE8EZGUKMyJUZhz+NAYHHKau/rY19HLvvY+Gjt62Nee\neN3YngiQuuYu1u1sZn9nHyPdE7QwJ3YwKA40TZXNyKF8RjAdzDsWp9yGFgZmVgBE3L09mH4zcMOw\n1e4Frjezn5DoOG5Vf4GIpINoUvMUFWOvOzA4RFNXXyIk2g+FxYHphvZeNu5u46G2hhGbqXJiEUrj\nOVyzYiEfeN2iUPYnzCODcuCeIM1iwO3u/jszWwng7rcA95E4rXQziVNL3xtiPSIiKRGLRoJf+qN3\nhB/Q0TtAQ1sPDUFIHJxu66E0nhNajRrPQERkGjjceAa6s5SIiCgMREREYSAiIigMREQEhYGIiKAw\nEBERFAYiIoLCQERESMOLzsysEdh+hG+fDYw2tkK6yrR9yrT9gczbp0zbH8i8fRppfxa4e+lob0i7\nMDgaZlY71hV46SjT9inT9gcyb58ybX8g8/bpSPZHzUQiIqIwEBGR6RcGw0daywSZtk+Ztj+QefuU\nafsDmbdPE96fadVnICIiI5tuRwYiIjIChYGIiEyfMDCzvzGzF8xss5l9JtX1TAYz22Zmz5jZOjNL\nuxF/zOy7ZtZgZhuS5s00swfMbFPwXJLKGidqlH36gpntCj6ndWZ2cSprnAgzqzSzP5rZs2a20cw+\nHsxPy89pjP1J588o18yeMLP1wT79SzD/ODNbHXzn/dTMssfcznToMzCzKPAi8CagDngSuMrdn01p\nYUfJzLYBNe6elhfLmNnrgQ7gB+5+cjDvP4Amd/+3ILRL3P1/p7LOiRhln74AdLj7V1JZ25EwsznA\nHHdfa2ZxYA3wd8A1pOHnNMb+XEH6fkYGFLh7h5llAY8CHwc+Cdzt7j8xs1uA9e5+82jbmS5HBmcC\nm919q7v3AT8BLk1xTdOeu/8JaBo2+1LgtmD6NhL/UNPGKPuUttx9j7uvDabbgeeAeaTp5zTG/qQt\nT+gIXmYFDwfOB+4M5h/2M5ouYTAP2Jn0uo40/x8g4MD9ZrbGzK5NdTGTpNzd9wTTe4HyVBYzia43\ns6eDZqS0aFIZzswWAsuB1WTA5zRsfyCNPyMzi5rZOqABeADYArS4+0CwymG/86ZLGGSqc9z9dOAi\n4LqgiSJjeKINMxPaMW8GFgPLgD3Af6W2nIkzs0LgLuAT7t6WvCwdP6cR9ietPyN3H3T3ZcB8Ei0h\nJ0x0G9MlDHYBlUmv5wfz0pq77wqeG4B7SPxPkO7qg3bdA+27DSmu56i5e33wj3UIuJU0+5yCdui7\ngB+7+93B7LT9nEban3T/jA5w9xbgj8BZQLGZxYJFh/3Omy5h8CRQHfSuZwNXAvemuKajYmYFQQcY\nZlYAvBnYMPa70sK9wHuC6fcAv0xhLZPiwJdm4G2k0ecUdE5+B3jO3b+atCgtP6fR9ifNP6NSMysO\npvNInCjzHIlQuDxY7bCf0bQ4mwggOFXsf4Ao8F13/2KKSzoqZraIxNEAQAy4Pd32yczuAM4lcbvd\neuDzwC+AnwFVJG5VfoW7p02H7Cj7dC6J5gcHtgEfSmpvn9LM7Bzgz8AzwFAw+59JtLOn3ec0xv5c\nRfp+RqeS6CCOkviB/zN3vyH4jvgJMBN4Crja3XtH3c50CQMRERnddGkmEhGRMSgMREREYSAiIgoD\nERFBYSAiIigMRF7BzAaT7l65bjLvcmtmC5PvaCoyVcQOv4rItNMdXNovMm3oyEBknILxI/4jGEPi\nCTNbEsxfaGYPBTc5e9DMqoL55WZ2T3Cf+fVmtiLYVNTMbg3uPX9/cNWoSEopDEReKW9YM9E7k5a1\nuvspwE0krmgH+Dpwm7ufCvwYuDGYfyPwiLufBpwObAzmVwPfcPeTgBbgspD3R+SwdAWyyDBm1uHu\nhSPM3wac7+5bg5ud7XX3WWa2j8SAKf3B/D3uPtvMGoH5ybcACG6b/IC7Vwev/zeQ5e7/Gv6eiYxO\nRwYiE+OjTE9E8v1hBlHfnUwBCgORiXln0vPjwfRfSNwJF+BdJG6EBvAg8GE4OPhI0bEqUmSi9ItE\n5JXyglGjDvidux84vbTEzJ4m8ev+qmDeR4HvmdmngUbgvcH8jwOrzOz9JI4APkxi4BSRKUd9BiLj\nFPQZ1Lj7vlTXIjLZ1EwkIiI6MhARER0ZiIgICgMREUFhICIiKAxERASFgYiIAP8f1ob4a5ilH6cA\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}